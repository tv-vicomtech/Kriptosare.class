{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = 'pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark')\n",
    "from pyspark import SparkContext,SparkConf\n",
    "conf = (SparkConf()\n",
    "         .setMaster(\"spark://10.200.5.39:7077\")\n",
    "         .set(\"spark.executor.memory\",\"30g\")\n",
    "         .set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "         .setAppName(\"Join\"))\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import binascii\n",
    "from pyspark.sql import SQLContext\n",
    "from functools import reduce\n",
    "from sklearn import tree\n",
    "from pyspark.sql.types import *\n",
    "import pygraphviz\n",
    "import pyspark.sql.functions as f\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\n",
    "from sklearn import datasets,metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from IPython.display import Image\n",
    "from networkx.drawing.nx_pydot import write_dot\n",
    "import pickle\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"hdfs://10.200.5.25:9001/user/titanium/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_label_to_id(df,new_col,old_col):\n",
    "    df = df.withColumn(new_col,f.when(f.col(old_col).like(\"b%\"),0).otherwise(f.col(old_col)))\n",
    "    df = df.withColumn(new_col,f.when(f.col(old_col).like(\"e%\"),1).otherwise(f.col(new_col)))\n",
    "    df = df.withColumn(new_col,f.when(f.col(old_col).like(\"c%\"),2).otherwise(f.col(new_col)))\n",
    "    df = df.withColumn(new_col,f.when(f.col(old_col).like(\"o%\"),3).otherwise(f.col(new_col)))\n",
    "    df = df.withColumn(new_col,f.when(f.col(old_col).like(\"Coin%\"),3).otherwise(f.col(new_col)))\n",
    "    return df\n",
    "\n",
    "def change_label_new_df(df,new_col,old_col,t):\n",
    "    df = df.withColumn(new_col,f.when(f.col(old_col).like(\"b%\"),f.concat(f.lit(t*\"b\"),f.col(old_col))).otherwise(f.col(old_col)))\n",
    "    df = df.withColumn(new_col,f.when(f.col(old_col).like(\"e%\"),f.concat(f.lit(t*\"e\"),f.col(old_col))).otherwise(f.col(new_col)))\n",
    "    df = df.withColumn(new_col,f.when(f.col(old_col).like(\"c%\"),f.concat(f.lit(t*\"c\"),f.col(old_col))).otherwise(f.col(new_col)))\n",
    "    df = df.withColumn(new_col,f.when(f.col(old_col).like(\"o%\"),f.concat(f.lit(t*\"o\"),f.col(old_col))).otherwise(f.col(new_col)))\n",
    "    return df\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"label\", StringType(), True),\n",
    "    StructField(\"user\", StringType(), True),\n",
    "])\n",
    "\n",
    "def join_dataframe_intag(DF,pref):\n",
    "    DF=DF.groupBy(\"user\").agg(f.collect_list(\"label\").alias(\"list\"))\n",
    "    \n",
    "    DF = DF.alias('a').join(df_label.alias('b'),f.col(\"a.user\")==f.col(\"b.label\"),\"leftouter\")\\\n",
    "    .select(\"b.class\",\"a.user\",\"a.list\")\n",
    "    DF = DF.withColumnRenamed(\"class\",\"label\")\n",
    "    #DF=transform_label_to_id(DF,\"label\",\"user\")\n",
    "    DF = DF.select(\"user\",f.explode(\"list\").alias(\"value\")).groupBy(\"user\",\"value\").agg(f.count(\"value\").alias(\"cnt\"))\n",
    "    DF2 = DF.groupBy(\"user\").agg(f.count(\"user\").alias(\"cnt\")).drop(\"cnt\")\n",
    "    DF2 = DF2.alias(\"a\").join(DF.alias('b'),(f.col(\"a.user\")==f.col(\"b.user\")) & (f.col(\"b.value\")==0),\"leftouter\").select(\"a.user\",\"b.cnt\").withColumnRenamed(\"cnt\",pref+\"_cnt0\")\n",
    "    DF2 = DF2.alias(\"a\").join(DF.alias('b'),(f.col(\"a.user\")==f.col(\"b.user\")) & (f.col(\"b.value\")==1),\"leftouter\").select(\"a.user\",pref+\"_cnt0\",\"b.cnt\").withColumnRenamed(\"cnt\",pref+\"_cnt1\")\n",
    "    DF2 = DF2.alias(\"a\").join(DF.alias('b'),(f.col(\"a.user\")==f.col(\"b.user\")) & (f.col(\"b.value\")==2),\"leftouter\").select(\"a.user\",pref+\"_cnt0\",pref+\"_cnt1\",\"b.cnt\").withColumnRenamed(\"cnt\",pref+\"_cnt2\")\n",
    "    DF2 = DF2.alias(\"a\").join(DF.alias('b'),(f.col(\"a.user\")==f.col(\"b.user\")) & (f.col(\"b.value\")==3),\"leftouter\").select(\"a.user\",pref+\"_cnt0\",pref+\"_cnt1\",pref+\"_cnt2\",\"b.cnt\").withColumnRenamed(\"cnt\",pref+\"_cnt3\")\n",
    "\n",
    "    DF2=DF2.fillna(0)\n",
    "    return DF2\n",
    "\n",
    "def link_outclass_intag(user,b):\n",
    "    dd = [(user[i][0], int(b[i])) for i in range(len(user))]\n",
    "    df = sqlContext.createDataFrame(sc.parallelize(dd),schema=[\"user\", \"label\"])\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Definition\n",
    "# 0: user\n",
    "# 1: exchange\n",
    "# 2: casino\n",
    "# 3: coinbase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_gambling = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "#.option('header','true')\\\n",
    "#.option(\"inferSchema\", \"true\")\\\n",
    "#.load(path+\"walletexp_data/gambling*\")\n",
    "\n",
    "#df_exchange = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "#.option('header','true')\\\n",
    "#.option(\"inferSchema\", \"true\")\\\n",
    "#.load(path+\"walletexp_data/exchange*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_exchange =df_exchange.withColumn(\"xxx\",f.lit(1))\n",
    "#df_gambling =df_gambling.withColumn(\"xxx\",f.lit(2))\n",
    "#df_label = df_exchange.union(df_gambling).groupby(\"label\").agg(f.first(\"xxx\").alias(\"class\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "#               IMPORT DATAFRAMES\n",
    "#########################################################################\n",
    "path_directory=\"part7/\"\n",
    "entity_data=[]\n",
    "address_data=[]\n",
    "for i in range(1,5):\n",
    "    entity_data2= sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "    .option('header','true')\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(path+path_directory+\"entity_feature/\")\n",
    "\n",
    "    address_data2 = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "    .option('header','true')\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(path+path_directory+\"address_feature/\")\n",
    "    if(i==1):\n",
    "        entity_data=entity_data2 \n",
    "        address_data=address_data2 \n",
    "    else:\n",
    "        entity_data=entity_data.union(entity_data2) \n",
    "        address_data=address_data.union(address_data2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_data=entity_data.randomSplit([0.7,0.3])\n",
    "address_data=address_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "#           STORE MOTIFS2 FEATURE IN HDFS\n",
    "#############################################################\n",
    "entity_data[0].write\\\n",
    ".format(\"com.databricks.spark.csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".save(path+pathDir+\"/entity_train\")\n",
    "\n",
    "entity_data[1].write\\\n",
    ".format(\"com.databricks.spark.csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".save(path+pathDir+\"/entity_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "#           STORE MOTIFS2 FEATURE IN HDFS\n",
    "#############################################################\n",
    "address_data[0].write\\\n",
    ".format(\"com.databricks.spark.csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".save(path+pathDir+\"/address_train\")\n",
    "\n",
    "address_data[1].write\\\n",
    ".format(\"com.databricks.spark.csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".save(path+pathDir+\"/address_test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
