{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.datastax.spark:spark-cassandra-connector_2.11:2.3.0 --conf spark.cassandra.connection.host=10.200.5.39 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark')\n",
    "from pyspark import SparkContext,SparkConf\n",
    "conf = (SparkConf()\n",
    "         .setMaster(\"spark://10.200.5.39:7077\")\n",
    "         .set(\"spark.executor.memory\",\"60g\")\n",
    "        .set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "         .setAppName(\"readapp\"))\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import binascii\n",
    "from pyspark.sql import SQLContext\n",
    "from functools import reduce\n",
    "import pygraphviz\n",
    "import pyspark.sql.functions as f\n",
    "from IPython.display import Image\n",
    "from networkx.drawing.nx_pydot import write_dot\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from Cassandra\n",
    "def load_and_get_table_df(keys_space_name, table_name):\n",
    "    table_df = sqlContext.read\\\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "        .options(table=table_name, keyspace=keys_space_name)\\\n",
    "        .load()\n",
    "    return table_df\n",
    "\n",
    "# Convert bytearray to str\n",
    "def using_str_format(test_obj) -> str:\n",
    "    return \"\".join(\"{:02x}\".format(x) for x in test_obj)\n",
    "\n",
    "# Retrive a user in the output address dataframe\n",
    "def user_find(x):\n",
    "    return df_output_addresses_tag_grpby_addr.where(f.col(\"address\")==str(x)).select(\"user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data, read from cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "df_blocks_original = load_and_get_table_df(\"graphsense_raw\", \"block\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blocks_original.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions_original = load_and_get_table_df(\"graphsense_raw\", \"transaction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/titanium/realdata/Blockchain_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Block data:\"+str(df_blocks_original.count())+\", column: \"+str(len(df_blocks_original.columns)))\n",
    "print(\"Transaction data:\"+str(df_transactions_original.count())+\", column: \"+str(len(df_transactions_original.columns)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split_col = f.split(df_blocks.select('txs'), ',')\n",
    "df_blocks = df_blocks_original.withColumn('txs',f.explode(f.col('txs')))\n",
    "df_blocks = df_blocks.withColumn('txs',f.hex(f.col('txs')))\n",
    "df_blocks = df_blocks.withColumn('block_hash',f.hex(f.col('block_hash')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trasform transaction dataframe in order to separate different value in VOUT field\n",
    "df_transactions = df_transactions_original.withColumn('tx_hash',f.hex(f.col('tx_hash')))\n",
    "df_transactions = df_transactions.withColumn('tx_id',f.hex(f.col('tx_id')))\n",
    "df_transactions = df_transactions.withColumn(\"vout\", f.explode(f.col('vout')))\n",
    "\n",
    "df_transactions = df_transactions.withColumn(\"amount\",df_transactions['vout']['value']) \n",
    "df_transactions = df_transactions.withColumn(\"vout_idx\",df_transactions['vout']['n']) \n",
    "df_transactions = df_transactions.withColumn(\"address\",f.explode(df_transactions['vout']['addresses'])) \n",
    "df_transactions = df_transactions.drop(\"vout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blocks.coalesce(1).write\\\n",
    ".format(\"com.databricks.spark.csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"codec\", \"org.apache.hadoop.io.compress.GzipCodec\")\\\n",
    ".save(path+\"block\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions.coalesce(1).write\\\n",
    ".format(\"com.databricks.spark.csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"codec\", \"org.apache.hadoop.io.compress.GzipCodec\")\\\n",
    ".save(path+\"block\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
