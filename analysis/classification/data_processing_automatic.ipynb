{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.datastax.spark:spark-cassandra-connector_2.11:2.3.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark')\n",
    "from pyspark import SparkContext,SparkConf\n",
    "sc = SparkContext()\n",
    "\n",
    "## With Cluster\n",
    "#conf = (SparkConf()\n",
    "#         .setMaster(\"spark://10.200.5.39:7077\")\n",
    "#         .set(\"spark.driver.host\",\"10.200.5.39\") \n",
    "#         .set(\"spark.executor.memory\",\"58g\")\n",
    "#         .set('spark.driver.memory', '60G')\n",
    "#         .setAppName(\"newanalysis\"))\n",
    "#sc = SparkContext(conf=conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import binascii\n",
    "from pyspark.sql import SQLContext\n",
    "from functools import reduce\n",
    "#import pygraphviz\n",
    "import pyspark.sql.functions as f\n",
    "from IPython.display import Image\n",
    "#from networkx.drawing.nx_pydot import write_dot\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Kriptosare.class/\"\n",
    "pathDir=\"/Kriptosare.class/analysis/analysis_rest/processing\"\n",
    "pathDirfinal=\"/Kriptosare.class/analysis/analysis_rest/final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from Cassandra\n",
    "def load_and_get_table_df(keys_space_name, table_name):\n",
    "    table_df = sqlContext.read\\\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "        .options(table=table_name, keyspace=keys_space_name)\\\n",
    "        .load()\n",
    "    return table_df\n",
    "\n",
    "# Convert bytearray to str\n",
    "def using_str_format(test_obj) -> str:\n",
    "    return \"\".join(\"{:02x}\".format(x) for x in test_obj)\n",
    "\n",
    "# Retrive a user in the output address dataframe\n",
    "def user_find(x):\n",
    "    return df_output_addresses_tag_grpby_addr.where(f.col(\"address\")==str(x)).select(\"user\")\n",
    "\n",
    "from IPython.display import HTML, Javascript, display, display_html\n",
    "import os\n",
    "\n",
    "def initialize():\n",
    "    display_html(\"<script>IPython.notebook.kernel.restart()</script>\",raw=True)\n",
    "    sc.stop()\n",
    "    !sleep 15\n",
    "    !jupyter nbconvert --execute data_processing_automatic.ipynb --ExecutePreprocessor.timeout=-1 \n",
    "    \n",
    "#    display(HTML(\n",
    "#        '''\n",
    "#            <script>\n",
    "#                code_show = false;\n",
    "#                function restart_run_all(){\n",
    "#                    IPython.notebook.kernel.restart();\n",
    "#                    setTimeout(function(){\n",
    "#                        %run MyOtherNotebook.ipynb;\n",
    "#                    }, 15000)\n",
    "#                }\n",
    "#            document.getElementById(\"demo\").innerHTML = restart_run_all();\n",
    "#            </script>\n",
    "#            <p id=\"demo\"></p>  \n",
    "#        '''\n",
    "#    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block1(i):\n",
    "    df_transactions2 = sqlContext.read\\\n",
    "             .format(\"csv\")\\\n",
    "             .option(\"header\", \"true\")\\\n",
    "             .load(path+\"/bitcoin/transaction_part\"+str(i))\n",
    "    ##############################################################\n",
    "    #           OUTPUT ADDRESS DATAFRAME CREATION\n",
    "    #############################################################\n",
    "    df_output_addresses = df_transactions2.select('address','vout_idx','amount','tx_id','tx_hash','size','coinbase','height')\n",
    "\n",
    "    #############################################################\n",
    "    #           REMOVE DUPLICATE OUTPUT DATAFRAME\n",
    "    #############################################################\n",
    "    df_output_addresses=df_output_addresses.dropDuplicates(['address','vout_idx','amount','tx_id'])\n",
    "    ##############################################################\n",
    "    #           OUTPUT ADDRESS DATAFRAME CREATION\n",
    "    #############################################################\n",
    "    df_output_addresses = df_transactions2.select('address','vout_idx','amount','tx_id','tx_hash','size','coinbase','height')\n",
    "\n",
    "    #############################################################\n",
    "    #           REMOVE DUPLICATE OUTPUT DATAFRAME\n",
    "    #############################################################\n",
    "    df_output_addresses=df_output_addresses.dropDuplicates(['address','vout_idx','amount','tx_id'])\n",
    "    \n",
    "    df_output_addresses_tag=df_output_addresses.alias('a')\\\n",
    "    .join(df_label.alias('b'),(f.col('a.address') == f.col('b.address')),\"leftouter\")\\\n",
    "    .select(f.col('b.user').alias(\"user\"),f.col('a.address'),f.col('a.amount'),f.col('a.coinbase'),f.col('a.tx_id'),f.col('a.vout_idx'),f.col('a.height'))\n",
    "    df_output_addresses_tag.write.parquet(pathDir+\"/df_output_addresses_tag_\"+str(i)+\".parquet\")\n",
    "\n",
    "    #############################################################\n",
    "    #   INPUT ADDRESS DATAFRAME CREATION AND REMOVE DUPLICATE\n",
    "    #############################################################\n",
    "    df_output_addresses_tag=sqlContext.read.parquet(pathDir+\"/df_output_addresses_tag_\"+str(i)+\".parquet\")\n",
    "\n",
    "    df_input_addresses=df_transactions2.dropDuplicates([\"vin_txid\", \"vin_vout\"])\n",
    "    df_input_addresses=df_input_addresses.alias('a')\\\n",
    "    .join(df_output_addresses.alias('b'),(f.col('a.vin_txid') == f.col('b.tx_id')) & (f.col('a.vin_vout') == f.col('b.vout_idx')),\"leftouter\")\\\n",
    "    .select(f.col('b.address'),f.col('a.vin_vout'),f.col('b.amount'),f.col('a.tx_id').alias('tx_id'),f.col('b.tx_hash'),f.col('b.coinbase'),f.col('b.height'),f.col('b.size'))\n",
    "    df_input_addresses_tag=df_input_addresses.alias('a')\\\n",
    "    .join(df_label.alias('b'),(f.col('b.address') == f.col('a.address')),\"leftouter\")\\\n",
    "    .select(f.col('b.user').alias(\"user\"),f.col('a.address'),f.col('a.amount'),f.col('a.coinbase'),f.col('a.tx_id'),f.col('a.vin_vout').alias('vout_idx'))\n",
    "    df_input_addresses_tag.write.parquet(pathDir+\"/df_input_addresses_tag_\"+str(i)+\".parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block2(i):\n",
    "    df_output_addresses_tag=sqlContext.read.parquet(pathDir+\"/df_output_addresses_tag_\"+str(i)+\".parquet\")\n",
    "    df_input_addresses_tag=sqlContext.read.parquet(pathDir+\"/df_input_addresses_tag_\"+str(i)+\".parquet\")\n",
    "    df_output_addresses_tag_copy =df_output_addresses_tag.groupby(\"tx_id\").agg(f.count(\"address\").alias(\"cnt_out\"))\n",
    "\n",
    "    tx_input_schema = df_input_addresses_tag.groupby(\"tx_id\").agg(f.count(\"address\").alias(\"cnt_in\"))\n",
    "\n",
    "    ########################################################################\n",
    "    df_output_addresses_tag_copy=df_output_addresses_tag_copy.alias('a')\\\n",
    "    .join(tx_input_schema.alias('b'),['tx_id'],'leftouter')\\\n",
    "    .select(f.col('b.cnt_in'),f.col('a.cnt_out'),f.col('a.tx_id'))\n",
    "\n",
    "    df_output_addresses_tag_copy.count()\n",
    "\n",
    "    df_output_addresses_tag = df_output_addresses_tag.alias(\"a\")\\\n",
    "    .join(df_output_addresses_tag_copy.alias('b'),['tx_id'],'leftouter')\\\n",
    "    .select(f.col('b.cnt_in'),f.col('b.cnt_out'),f.col('a.user'),f.col('a.address'),f.col('a.amount'),f.col('a.coinbase'),f.col('a.tx_id'),f.col('a.vout_idx'),f.col('a.height'))\n",
    "\n",
    "    first_show=df_output_addresses_tag.groupby(\"address\").agg(f.min(\"height\").alias(\"show\"))\n",
    "\n",
    "    df_output_addresses_tag = df_output_addresses_tag.alias(\"a\")\\\n",
    "    .join(first_show.alias('b'),['address'],'leftouter')\\\n",
    "    .select(f.col('b.show'),f.col('a.cnt_in'),f.col('a.cnt_out'),f.col('a.user'),f.col('a.address'),f.col('a.amount'),f.col('a.coinbase'),f.col('a.tx_id'),f.col('a.vout_idx'),f.col('a.height'))\n",
    "\n",
    "    df_output_addresses_tag.count()\n",
    "\n",
    "    df_output_addresses_tag=df_output_addresses_tag.withColumn(\"first\",f.when(f.col(\"show\")==f.col(\"height\"),1).otherwise(0)).drop(\"show\")\n",
    "\n",
    "    df_output_addresses_tag_reduced=df_output_addresses_tag.fillna(\"Unknow\",subset=[\"user\"])\n",
    "    df_input_addresses_tag_reduced=df_input_addresses_tag.fillna(\"Unknow\",subset=[\"user\"])\n",
    "    df_output_addresses_tag_reduced.write.parquet(pathDir+\"/df_output_addresses_tag_reduced_\"+str(i)+\".parquet\")\n",
    "    df_input_addresses_tag_reduced.write.parquet(pathDir+\"/df_input_addresses_tag_reduced_\"+str(i)+\".parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block3(i):\n",
    "    df_output_addresses_tag_reduced=sqlContext.read.parquet(pathDir+\"/df_output_addresses_tag_reduced_\"+str(i)+\".parquet\")\n",
    "    df_input_addresses_tag_reduced=sqlContext.read.parquet(pathDir+\"/df_input_addresses_tag_reduced_\"+str(i)+\".parquet\")\n",
    "    #############################################################\n",
    "    #   Aggregate input and output labeled dataframe for distinct address\n",
    "    #############################################################\n",
    "\n",
    "    df_output_addresses_tag_grpby_addr=df_output_addresses_tag_reduced.groupby(df_output_addresses_tag_reduced.address)\\\n",
    "    .agg(f.count('address').alias(\"count\"),(f.sum('amount')).alias(\"totamount\"),f.first(f.col(\"user\")).alias(\"user\"))\n",
    "    df_input_addresses_tag_grpby_addr=df_input_addresses_tag_reduced.groupby(df_input_addresses_tag_reduced.address)\\\n",
    "    .agg(f.count('address').alias(\"count\"),(f.sum('amount')).alias(\"totamount\"),f.first(f.col(\"user\")).alias(\"user\"))\n",
    "\n",
    "    df_output_addresses_tag_grpby_addr =df_output_addresses_tag_grpby_addr.withColumn(\"totamount\",f.round(f.col(\"totamount\"))/100000000)\n",
    "    df_input_addresses_tag_grpby_addr =df_input_addresses_tag_grpby_addr.withColumn(\"totamount\",f.round(f.col(\"totamount\"))/100000000)\n",
    "\n",
    "    df_output_addresses_tag_grpby_addr.write.parquet(pathDir+\"/df_output_addresses_tag_grpby_addr_\"+str(i)+\".parquet\")\n",
    "    df_input_addresses_tag_grpby_addr.write.parquet(pathDir+\"/df_input_addresses_tag_grpby_addr_\"+str(i)+\".parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block4(i):\n",
    "    df_output_addresses_tag_grpby_addr=sqlContext.read.parquet(pathDir+\"/df_output_addresses_tag_grpby_addr_\"+str(i)+\".parquet\")\n",
    "    df_input_addresses_tag_grpby_addr=sqlContext.read.parquet(pathDir+\"/df_input_addresses_tag_grpby_addr_\"+str(i)+\".parquet\")\n",
    "\n",
    "\n",
    "    #############################################################\n",
    "    # Aggregate input and output labeled dataframe for distinct user\n",
    "    #############################################################\n",
    "\n",
    "    df_output_addresses_tag_grpby_user=df_output_addresses_tag_grpby_addr.groupby(df_output_addresses_tag_grpby_addr.user)\\\n",
    "    .agg(f.count('address').alias(\"naddress\"),f.sum('totamount').alias(\"balancerecv\"))\n",
    "    df_input_addresses_tag_grpby_user=df_input_addresses_tag_grpby_addr.groupby(df_input_addresses_tag_grpby_addr.user)\\\n",
    "    .agg(f.count('address').alias(\"naddress\"),f.sum('totamount').alias(\"balancesend\"))\n",
    "\n",
    "\n",
    "    df_input_addresses_tag_grpby_user=df_input_addresses_tag_grpby_user.where(f.col(\"user\")!=\"Unknow\")\n",
    "    df_output_addresses_tag_grpby_user=df_output_addresses_tag_grpby_user.where(f.col(\"user\")!=\"Unknow\")\n",
    "\n",
    "    df_output_addresses_tag_grpby_user.write.parquet(pathDir+\"/df_output_addresses_tag_grpby_user_\"+str(i)+\".parquet\")\n",
    "    df_input_addresses_tag_grpby_user.write.parquet(pathDir+\"/df_input_addresses_tag_grpby_user_\"+str(i)+\".parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block5(i):\n",
    "    df_output_addresses_tag_grpby_user=sqlContext.read.parquet(pathDir+\"/df_output_addresses_tag_grpby_user_\"+str(i)+\".parquet\")\n",
    "    df_input_addresses_tag_grpby_user=sqlContext.read.parquet(pathDir+\"/df_input_addresses_tag_grpby_user_\"+str(i)+\".parquet\")\n",
    "\n",
    "    df_input_addresses_tag_grpby_addr=sqlContext.read.parquet(pathDir+\"/df_input_addresses_tag_grpby_addr_\"+str(i)+\".parquet\")\n",
    "    df_output_addresses_tag_grpby_addr=sqlContext.read.parquet(pathDir+\"/df_output_addresses_tag_grpby_addr_\"+str(i)+\".parquet\")\n",
    "\n",
    "    #############################################################\n",
    "    #           BALANCE ESTIMATION\n",
    "    #############################################################\n",
    "    # Retrive user in label dataframe that are not present into the input/output dataframe\n",
    "    list_unique_input_user=df_input_addresses_tag_grpby_user.groupby(\"user\").agg(f.first(\"user\").alias(\"unique\")).drop(\"user\")\n",
    "    list_unique_output_user=df_output_addresses_tag_grpby_user.groupby(\"user\").agg(f.first(\"user\").alias(\"unique\")).drop(\"user\")\n",
    "\n",
    "    # Add retrived user into the input/output dataframe (with default parameters)\n",
    "    user_out_toadd=list_unique_input_user.alias(\"a\").join(list_unique_output_user.alias(\"b\"),f.col(\"a.unique\")==f.col(\"b.unique\"),\"left_anti\")\n",
    "    user_in_toadd=list_unique_output_user.alias(\"a\").join(list_unique_input_user.alias(\"b\"),f.col(\"a.unique\")==f.col(\"b.unique\"),\"left_anti\")\n",
    "\n",
    "    #Add missing user in the input and output dataframe in order to calculate an estimation of the balance\n",
    "    user_in_toadd = user_in_toadd.withColumn(\"naddress\", f.lit(0))\n",
    "    user_in_toadd = user_in_toadd.withColumn(\"balancein\", f.lit(0))\n",
    "\n",
    "    df_input_addresses_tag_grpby_user_filled = df_input_addresses_tag_grpby_user.union(user_in_toadd)\n",
    "\n",
    "    user_out_toadd = user_out_toadd.withColumn(\"naddress\", f.lit(0))\n",
    "    user_out_toadd = user_out_toadd.withColumn(\"balanceout\", f.lit(0))\n",
    "    df_output_addresses_tag_grpby_user_filled = df_output_addresses_tag_grpby_user.union(user_out_toadd)\n",
    "\n",
    "\n",
    "    #############################################################\n",
    "    #           BALANCE ESTIMATION\n",
    "    #############################################################\n",
    "\n",
    "    df_user_balance=df_output_addresses_tag_grpby_user_filled.alias('a')\\\n",
    "    .join(df_input_addresses_tag_grpby_user_filled.alias('b'),\"user\",\"leftouter\")\\\n",
    "    .select(f.col('a.user'),f.col('a.balancerecv'),f.col('b.balancesend'))\n",
    "    df_user_balance=df_user_balance.fillna(0,subset=[\"balancerecv\",\"balancesend\"])\n",
    "    df_user_balance=df_user_balance.withColumn(\"balance\",f.col(\"balancerecv\")-f.col(\"balancesend\"))\\\n",
    "    .sort(f.col(\"balance\").desc())\n",
    "    df_user_balance = df_user_balance.withColumn(\"balance\",f.when(f.abs(f.col(\"balance\"))<0.00000001,0).otherwise(f.col(\"balance\")))\n",
    "\n",
    "    df_user_balance.count()\n",
    "\n",
    "    df_user_balance.write.parquet(pathDirfinal+\"/df_user_balance_\"+str(i)+\".parquet\")\n",
    "\n",
    "    #############################################################\n",
    "    #           COMPUTE ADDRESS FEATURE\n",
    "    #############################################################\n",
    "\n",
    "    address_feature=df_output_addresses_tag_grpby_addr.alias(\"a\")\\\n",
    "    .join(df_input_addresses_tag_grpby_addr.alias(\"b\"),f.col(\"a.address\")==f.col(\"b.address\"),\"outer\")\\\n",
    "    .select(f.col('a.address').alias(\"a1\"),f.col('b.address').alias(\"b1\"),f.col('a.user').alias(\"a2\"),f.col('b.user').alias(\"b2\"),f.col('a.count').alias(\"count_rec\"),f.col('a.totamount').alias(\"totamount_rec\"),f.col('b.count').alias(\"count_sent\"),f.col('b.totamount').alias(\"totamount_sent\"))\n",
    "    address_feature=address_feature.withColumn(\"address\",f.when(f.col(\"a1\").isNotNull(),f.col(\"a1\")).otherwise(f.col(\"b1\")))\\\n",
    "    .drop(\"a1\",\"b1\")\n",
    "    address_feature=address_feature.withColumn(\"user\",f.when(f.col(\"a2\").isNotNull(),f.col(\"a2\")).otherwise(f.col(\"b2\")))\\\n",
    "    .drop(\"a2\",\"b2\")\n",
    "    address_feature=address_feature.fillna(0)\n",
    "    address_feature=address_feature.withColumn(\"balance\",f.col(\"totamount_rec\")-f.col(\"totamount_sent\"))\n",
    "    address_feature=address_feature.withColumn(\"unique\",f.when((f.col(\"count_rec\")<2)&(f.col(\"count_sent\")<2),1).otherwise(0))\n",
    "\n",
    "    address_feature=address_feature.alias(\"a\")\\\n",
    "    .join(df_output_addresses_tag_grpby_user.alias(\"b\"),f.col(\"a.user\")==f.col(\"b.user\"),\"leftouter\")\\\n",
    "    .select(f.col('a.address'),f.col('a.user'),f.col('a.count_rec'),f.col(\"totamount_rec\"),f.col('a.count_sent'),f.col('a.totamount_sent'),f.col('a.balance'),f.col('a.unique'),f.col('b.naddress').alias('sibling'))\n",
    "    address_feature=address_feature.fillna(0)\n",
    "    address_feature=address_feature.where(f.col(\"user\")!=\"Unknow\")\n",
    "    #address_feature.count()\n",
    "    address_feature=address_feature.withColumn(\"height\",f.lit(i))\n",
    "    address_feature.write.parquet(pathDirfinal+\"/address_feature_\"+str(i)+\".parquet\")\n",
    "\n",
    "\n",
    "def block6(i):\n",
    "    df_output_addresses_tag_reduced=sqlContext.read.parquet(pathDir+\"/df_output_addresses_tag_reduced_\"+str(i)+\".parquet\")\n",
    "    df_input_addresses_tag_reduced=sqlContext.read.parquet(pathDir+\"/df_input_addresses_tag_reduced_\"+str(i)+\".parquet\")\n",
    "    df_user_balance=sqlContext.read.parquet(pathDirfinal+\"/df_user_balance_\"+str(i)+\".parquet\")\n",
    "\n",
    "    #############################################################\n",
    "    #           COMPUTE ENTITY FEATURE\n",
    "    #############################################################\n",
    "    entity_feature=df_user_balance.alias(\"a\")\\\n",
    "    .join(df_output_addresses_tag_reduced.groupBy(\"user\").agg(f.countDistinct(\"address\").alias(\"add_recv\"),f.countDistinct(\"tx_id\").alias(\"count_recv\")).alias(\"b\"),f.col(\"a.user\")==f.col(\"b.user\"),\"leftouter\")\\\n",
    "    .select(f.col('a.user'),f.col('a.balancerecv'),f.col('a.balancesend'),f.col('a.balance'),f.col('b.count_recv'),f.col('b.add_recv'))\n",
    "    entity_feature=entity_feature.alias(\"a\")\\\n",
    "    .join(df_input_addresses_tag_reduced.groupBy(\"user\").agg(f.countDistinct(\"address\").alias(\"add_sent\"),f.countDistinct(\"tx_id\").alias(\"count_sent\")).alias(\"b\"),f.col(\"a.user\")==f.col(\"b.user\"),\"leftouter\")\\\n",
    "    .select(f.col('a.user'),f.col('a.balancerecv'),f.col('a.balancesend'),f.col('a.balance'),f.col('a.count_recv'),f.col('b.count_sent'),f.col('a.add_recv'),f.col('b.add_sent'))\n",
    "\n",
    "    entity_feature=entity_feature.fillna(0,subset=[\"count_recv\",\"count_sent\"])\n",
    "    entity_feature=entity_feature.where(f.col(\"user\")!=\"Unknow\")\n",
    "\n",
    "    entity_feature.count()\n",
    "    entity_feature=entity_feature.withColumn(\"height\",f.lit(i))\n",
    "    entity_feature.write.parquet(pathDirfinal+\"/entity_feature_\"+str(i)+\".parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block7(i):\n",
    "    df_transactions2 = sqlContext.read\\\n",
    "             .format(\"csv\")\\\n",
    "             .option(\"header\", \"true\")\\\n",
    "             .load(path+\"/bitcoin/transaction_part\"+str(i))\n",
    "    \n",
    "    df_transactions_general = df_transactions2.alias('a').join(df_label.alias('b'),f.col('a.address')==f.col('b.address'),\"leftouter\")\\\n",
    "    .select(f.col(\"a.height\"),f.col(\"a.coinbase\"),f.col(\"a.timestamp\"),f.col(\"a.tx_id\"),f.col(\"a.tx_number\"),f.col(\"a.address\"),f.col(\"a.amount\"),f.col(\"a.vout_idx\"),f.col(\"a.vin_txid\"),f.col(\"a.vin_vout\"),f.col(\"b.user\").alias(\"outuser\"))\n",
    "    df_transactions_general.count()\n",
    "\n",
    "    #Calculate the input amount of each transaction from each inuser\n",
    "    df_transactions_general_join_amount = df_transactions_general.groupBy(\"tx_id\",\"address\",\"vout_idx\").agg(f.first(\"amount\").alias(\"unique_amount\"),f.first(\"outuser\").alias(\"inuser\"))\n",
    "\n",
    "    #Join amount information with the basic dataframe information\n",
    "    df_transactions_general_information = df_transactions_general.alias('a').join(df_transactions_general_join_amount.alias('b'),(f.col('a.vin_txid')==f.col('b.tx_id'))&(f.col('a.vin_vout')==f.col('b.vout_idx')),\"leftouter\")\\\n",
    "    .select(f.col(\"a.height\"),f.col(\"a.coinbase\"),f.col(\"a.timestamp\"),f.col(\"a.tx_id\"),f.col(\"a.amount\"),f.col(\"a.outuser\"),f.col(\"a.address\"),f.col(\"a.vin_txid\"),f.col(\"a.vin_vout\"),f.col(\"b.unique_amount\").alias(\"amount_sent\"),f.col(\"b.address\").alias(\"address_sent\"),f.col(\"b.inuser\").alias(\"inuser_old\"))\n",
    "\n",
    "    #Remove outuser with null field\n",
    "    df_transactions_general_information = df_transactions_general_information.filter(f.col(\"outuser\").isNotNull())\n",
    "\n",
    "    #Remove substitute inuser null information with Coinbase information\n",
    "    df_transactions_general_information = df_transactions_general_information.withColumn(\"inuser\",f.when((f.col(\"inuser_old\").isNull())&(f.col(\"address\").isNotNull()),\"Coinbase\").otherwise(f.col(\"inuser_old\")))\n",
    "    df_transactions_general_information = df_transactions_general_information.drop(f.col(\"inuser_old\")).cache()\n",
    "    df_transactions_general_information.write.parquet(pathDir+\"/df_transactions_general_information_\"+str(i)+\".parquet\")\n",
    "\n",
    "def block8(i):\n",
    "    df_transactions_general_information=sqlContext.read.parquet(pathDir+\"/df_transactions_general_information_\"+str(i)+\".parquet\")\n",
    "    #Inuser-Outuser dataframe with count distinct transaction\n",
    "    df_inuser_outuser_numtx = df_transactions_general_information.groupby(\"outuser\",\"inuser\").agg(f.countDistinct(\"tx_id\"))\n",
    "\n",
    "    #############################################################\n",
    "    #           COMPUTE MOTIFS1 FEATURE\n",
    "    #############################################################\n",
    "\n",
    "    motifs_1 = df_transactions_general_information.groupBy(\"outuser\",\"inuser\",'tx_id').agg(f.countDistinct(\"address\").alias(\"address_recv_dist\"))\\\n",
    "    .select(\"outuser\",\"inuser\",'tx_id',\"address_recv_dist\").cache()\n",
    "\n",
    "    #Calculate out amount of each user (in-out) in each transactions\n",
    "    amount_out_processing = df_transactions_general_information.groupBy(\"outuser\",\"inuser\",\"tx_id\",\"address\").agg(f.first(\"amount\").alias(\"amount_recv\"))\\\n",
    "    .groupBy(\"outuser\",\"inuser\",\"tx_id\").agg(f.sum(\"amount_recv\").alias(\"amount_recv\"))\n",
    "    amount_out_processing=amount_out_processing.fillna(\"Unknow\")\n",
    "    #Calculate in amount of each user (in-out) in each transactions\n",
    "    amount_in_processing = df_transactions_general_information.groupBy(\"outuser\",\"inuser\",\"tx_id\").agg(f.count(\"vin_txid\").alias(\"tx_sent\"),f.sum(\"amount_sent\").alias(\"amount_sent\"),f.countDistinct(\"address_sent\").alias(\"address_sent\"))\\\n",
    "    .groupBy(\"outuser\",\"inuser\",\"tx_id\").agg(f.sum(\"tx_sent\").alias(\"tx_sent\"),f.sum(\"amount_sent\").alias(\"amount_sent\"),f.sum(\"address_sent\").alias(\"address_sent_dist\"))\n",
    "    amount_in_processing=amount_in_processing.fillna(\"Unknow\")\n",
    "\n",
    "\n",
    "    #Calculate out amount of each transactions\n",
    "    amount_out_processing_tx = amount_out_processing.groupBy(\"tx_id\").agg(f.sum(\"amount_recv\").alias(\"total_recv_amount\"))\n",
    "    #Calculate in amount of each transactions\n",
    "    amount_in_processing_tx = df_transactions_general_information.groupBy(\"tx_id\",\"vin_txid\",\"vin_vout\").agg(f.first(\"amount_sent\").alias(\"amount_sent\"))\\\n",
    "    .groupBy(\"tx_id\").agg(f.sum(\"amount_sent\").alias(\"total_sent_amount\"))\n",
    "    amount_out_processing=amount_out_processing.fillna(\"Unknow\")\n",
    "    amount_out_processing.count()\n",
    "\n",
    "    #Calculate fee in each transaction\n",
    "    fee_tx = amount_out_processing_tx.alias('a').join(amount_in_processing_tx.alias('b'), f.col(\"a.tx_id\")==f.col(\"b.tx_id\"))\\\n",
    "    .select(\"a.tx_id\",\"total_recv_amount\",\"total_sent_amount\")\n",
    "    fee_tx = fee_tx.withColumn(\"fees\",f.col(\"total_sent_amount\")-f.col(\"total_recv_amount\"))\n",
    "\n",
    "    #Join all dataframe information to a unique dataframe for motifs-1\n",
    "    motifs_1 = motifs_1.alias(\"a\").join(amount_out_processing.alias(\"b\"),(f.col(\"a.outuser\")==f.col(\"b.outuser\"))&(f.col(\"a.inuser\")==f.col(\"b.inuser\"))&(f.col(\"a.tx_id\")==f.col(\"b.tx_id\")))\\\n",
    "    .select(\"a.outuser\",\"a.inuser\",\"a.tx_id\",\"address_recv_dist\",\"amount_recv\")\n",
    "    motifs_1 = motifs_1.alias(\"a\").join(amount_in_processing.alias(\"b\"),(f.col(\"a.outuser\")==f.col(\"b.outuser\"))&(f.col(\"a.inuser\")==f.col(\"b.inuser\"))&(f.col(\"a.tx_id\")==f.col(\"b.tx_id\")))\\\n",
    "    .select(\"a.outuser\",\"a.inuser\",\"a.tx_id\",\"a.address_recv_dist\",\"a.amount_recv\",\"b.tx_sent\",\"b.address_sent_dist\",\"amount_sent\")\n",
    "    motifs_1=motifs_1.alias('a').join(motifs_1.groupBy(\"outuser\",\"inuser\").agg(f.countDistinct(\"tx_id\").alias(\"tx_recv_tot\")).fillna(\"Unknow\").alias('b'),(f.col(\"a.outuser\")==f.col(\"b.outuser\"))&(f.col(\"a.inuser\")==f.col(\"b.inuser\")))\\\n",
    "    .select(\"a.outuser\",\"a.inuser\",\"a.tx_id\",\"a.address_recv_dist\",\"a.amount_recv\",\"a.tx_sent\",\"a.address_sent_dist\",\"a.amount_sent\",\"tx_recv_tot\")\n",
    "    motifs_1=motifs_1.alias('a').join(fee_tx.alias('b'),(f.col(\"a.tx_id\")==f.col(\"b.tx_id\")))\\\n",
    "    .select(\"a.outuser\",\"a.inuser\",\"a.tx_id\",\"a.address_recv_dist\",\"a.amount_recv\",\"a.tx_sent\",\"a.address_sent_dist\",\"a.amount_sent\",\"a.tx_recv_tot\",\"b.fees\")\n",
    "\n",
    "    #Define relation between user loop or direct\n",
    "    motifs_1 = motifs_1.withColumn(\"loop_in_out\", f.when(f.col(\"outuser\")==f.col(\"inuser\"),1).otherwise(0))\n",
    "    motifs_1 = motifs_1.withColumn(\"direct_in_out\", f.when(f.col(\"outuser\")==f.col(\"inuser\"),0).otherwise(1)).cache()\n",
    "\n",
    "    #Set to 0 where find null\n",
    "    motifs_1 = motifs_1.fillna(0,subset=['amount_sent','fees'])\n",
    "    motifs_1.write.parquet(pathDirfinal+\"/motifs1_\"+str(i)+\".parquet\")\n",
    "        \n",
    "\n",
    "def block9(i):\n",
    "    df_transactions2 = sqlContext.read\\\n",
    "         .format(\"csv\")\\\n",
    "         .option(\"header\", \"true\")\\\n",
    "         .load(path+\"/bitcoin/transaction_part\"+str(i))\n",
    "    \n",
    "    df_transactions_general = df_transactions2.alias('a').join(df_label.alias('b'),f.col('a.address')==f.col('b.address'),\"leftouter\")\\\n",
    "    .select(f.col(\"a.height\"),f.col(\"a.coinbase\"),f.col(\"a.timestamp\"),f.col(\"a.tx_id\"),f.col(\"a.tx_number\"),f.col(\"a.address\"),f.col(\"a.amount\"),f.col(\"a.vout_idx\"),f.col(\"a.vin_txid\"),f.col(\"a.vin_vout\"),f.col(\"b.user\").alias(\"outuser\")).cache()\n",
    "    df_transactions_general.count()\n",
    "\n",
    "    #Create dataframe with all information without repeating, and remove \"null\" user (clone)\n",
    "    df_transactions_general_join_motifs2 = df_transactions_general.groupBy(\"tx_id\",\"address\",\"vout_idx\")\\\n",
    "    .agg(f.first(\"outuser\").alias(\"miduser\"),f.first(\"vin_txid\").alias(\"vin_txid\"),f.first(\"vin_vout\").alias(\"vin_vout\"))\n",
    "    #.agg(f.first(\"amount\").alias(\"unique_amount\"),f.first(\"outuser\").alias(\"miduser\"),f.first(\"vin_txid\").alias(\"vin_txid\"),f.first(\"vin_vout\").alias(\"vin_vout\"))\n",
    "    #df_transactions_general_join_motifs2 = df_transactions_general_join_motifs2.filter(f.col(\"miduser\").isNotNull())\n",
    "\n",
    "\n",
    "    #Join the previuos dataframe with the dataframe general in order to obtain 1-motifs\n",
    "    df_transactions_general_information2 = df_transactions_general.alias('a').join(df_transactions_general_join_motifs2.alias('b'),(f.col('a.vin_txid')==f.col('b.tx_id'))&(f.col('a.vin_vout')==f.col('b.vout_idx')),\"leftouter\")\\\n",
    "    .select(f.col(\"a.tx_id\"),f.col(\"a.outuser\"),f.col(\"a.address\"),f.col(\"a.vin_txid\").alias(\"tx_id_mid\"),f.col(\"a.vin_vout\").alias(\"vin_vout_idx_mid\"),f.col(\"b.vin_txid\").alias(\"tx_id_in\"),f.col(\"b.vin_vout\").alias(\"vin_vout_idx_in\"),f.col(\"b.miduser\"))\n",
    "    #.select(f.col(\"a.height\"),f.col(\"a.coinbase\"),f.col(\"a.timestamp\"),f.col(\"a.tx_id\"),f.col(\"a.amount\"),f.col(\"a.outuser\"),f.col(\"a.address\"),f.col(\"a.vin_txid\").alias(\"tx_id_mid\"),f.col(\"a.vin_vout\").alias(\"vin_vout_idx_mid\"),f.col(\"b.vin_txid\").alias(\"tx_id_in\"),f.col(\"b.vin_vout\").alias(\"vin_vout_idx_in\"),f.col(\"b.unique_amount\").alias(\"amount_mid\"),f.col(\"b.address\").alias(\"address_mid\"),f.col(\"b.miduser\"))\n",
    "\n",
    "    #Repeat the previuos operation in order to obtain 2-motifs\n",
    "    df_transactions_general_info_deep = df_transactions_general_information2.alias('a').join(df_transactions_general_join_motifs2.alias('b'),(f.col('a.tx_id_in')==f.col('b.tx_id'))&(f.col('a.vin_vout_idx_in')==f.col('b.vout_idx')),\"leftouter\")\\\n",
    "    .select(f.col(\"a.tx_id\"),f.col(\"a.outuser\"),f.col(\"a.address\"),f.col(\"a.tx_id_mid\"),f.col(\"a.miduser\"),f.col(\"b.miduser\").alias(\"inuser_old\"))\n",
    "    #.select(f.col(\"a.height\"),f.col(\"a.timestamp\"),f.col(\"a.tx_id\"),f.col(\"a.amount\"),f.col(\"a.outuser\"),f.col(\"a.address\"),f.col(\"a.tx_id_mid\"),f.col(\"a.vin_vout_idx_mid\"),f.col(\"a.amount_mid\"),f.col(\"a.address_mid\"),f.col(\"a.miduser\"),f.col(\"b.unique_amount\").alias(\"amount_sent\"),f.col(\"b.address\").alias(\"address_sent\"),f.col(\"b.miduser\").alias(\"inuser_old\"))\n",
    "\n",
    "    #Remove null user\n",
    "    df_transactions_general_info_deep = df_transactions_general_info_deep.filter(f.col(\"outuser\").isNotNull())\n",
    "    df_transactions_general_info_deep = df_transactions_general_info_deep.filter(f.col(\"miduser\").isNotNull())\n",
    "\n",
    "    #Change null user but with address with \"Coinbase\"\n",
    "    df_transactions_general_info_deep = df_transactions_general_info_deep.withColumn(\"inuser\",f.when((f.col(\"inuser_old\").isNull())&(f.col(\"address\").isNotNull()),\"Coinbase\").otherwise(f.col(\"inuser_old\")))\n",
    "    df_transactions_general_info_deep = df_transactions_general_info_deep.drop(f.col(\"inuser_old\"))\n",
    "\n",
    "    #Creating unique dataframe with outuser->tx->miduser->tx->inuser\n",
    "    motifs_2 = df_transactions_general_info_deep.groupBy(\"outuser\",\"miduser\",\"inuser\",\"tx_id\",\"tx_id_mid\")\\\n",
    "    .agg(f.count(\"address\"))\\\n",
    "    .select(\"outuser\",\"tx_id\",\"miduser\",\"tx_id_mid\",\"inuser\")\n",
    "\n",
    "\n",
    "    motifs_2.write.parquet(pathDir+\"/motifs_2notfinish_\"+str(i)+\".parquet\")\n",
    "    \n",
    "    \n",
    "def block10(i):\n",
    "    motifs_2 =sqlContext.read.parquet(pathDir+\"/motifs_2notfinish_\"+str(i)+\".parquet\")\n",
    "    motifs_2 = motifs_2.withColumn(\"loop_mid_out\", f.when(f.col(\"outuser\")==f.col(\"miduser\"),1).otherwise(0))\n",
    "    motifs_2 = motifs_2.withColumn(\"loop_in_mid\", f.when(f.col(\"miduser\")==f.col(\"inuser\"),1).otherwise(0))\n",
    "    motifs_2 = motifs_2.withColumn(\"loop_in_out\", f.when(f.col(\"outuser\")==f.col(\"inuser\"),1).otherwise(0))\n",
    "\n",
    "    motifs_2 = motifs_2.withColumn(\"direct_mid_out\", f.when(f.col(\"outuser\")==f.col(\"miduser\"),0).otherwise(1))\n",
    "    motifs_2 = motifs_2.withColumn(\"direct_in_mid\", f.when(f.col(\"miduser\")==f.col(\"inuser\"),0).otherwise(1))\n",
    "    motifs_2 = motifs_2.withColumn(\"direct_in_out\", f.when(f.col(\"outuser\")==f.col(\"inuser\"),0).otherwise(1))\n",
    "\n",
    "    motifs_1=sqlContext.read.parquet(pathDirfinal+\"/motifs1_\"+str(i)+\".parquet\")\n",
    "\n",
    "    motifs_2_cloned = motifs_2.toDF(\"outuser\",\"tx_id\",\"miduser\",\"tx_id_mid\",\"inuser\",\"loop_mid_out\",\"loop_in_mid\",\"loop_in_out\",\"direct_mid_out\",\"direct_in_mid\",\"direct_in_out\")\n",
    "\n",
    "    #Rename correctly the column\n",
    "    motifs_2_cloned= motifs_2_cloned.alias(\"a\").join(motifs_1.alias(\"b\"),(f.col(\"a.outuser\")==f.col(\"b.outuser\"))&(f.col(\"a.miduser\")==f.col(\"b.inuser\"))&(f.col(\"a.tx_id\")==f.col(\"b.tx_id\")),\"leftouter\")\\\n",
    "    .select(\"a.outuser\",\"a.tx_id\",\"b.address_recv_dist\",\"b.amount_recv\",\"b.fees\",\"b.tx_sent\",\"b.address_sent_dist\",\"b.amount_sent\",\"a.miduser\",\"a.tx_id_mid\",\"a.inuser\",\"a.loop_mid_out\",\"a.loop_in_mid\",\"a.loop_in_out\",\"a.direct_mid_out\",\"a.direct_in_mid\",\"a.direct_in_out\")\\\n",
    "    .withColumnRenamed(\"fees\",\"fee2\")\\\n",
    "    .withColumnRenamed(\"address_recv_dist\",\"address_recv_dist_to_out\")\\\n",
    "    .withColumnRenamed(\"amount_recv\",\"amount_recv_to_out\")\\\n",
    "    .withColumnRenamed(\"tx_sent\",\"tx_sent_from_mid\")\\\n",
    "    .withColumnRenamed(\"address_sent_dist\",\"address_sent_from_mid\")\\\n",
    "    .withColumnRenamed(\"amount_sent\",\"amount_sent_from_mid\").cache()\n",
    "\n",
    "\n",
    "    motifs_2_cloned= motifs_2_cloned.alias(\"a\").join(motifs_1.alias(\"b\"),(f.col(\"a.inuser\")==f.col(\"b.inuser\"))&(f.col(\"a.miduser\")==f.col(\"b.outuser\"))&(f.col(\"a.tx_id_mid\")==f.col(\"b.tx_id\")),\"leftouter\")\\\n",
    "    .select(\"a.outuser\",\"a.tx_id\",\"a.address_recv_dist_to_out\",\"a.amount_recv_to_out\",\"a.fee2\",\"a.tx_sent_from_mid\",\"a.address_sent_from_mid\",\"a.amount_sent_from_mid\",\"a.miduser\",\"a.tx_id_mid\",\"b.address_recv_dist\",\"b.amount_recv\",\"b.tx_sent\",\"b.address_sent_dist\",\"b.amount_sent\",\"b.fees\",\"a.inuser\",\"a.loop_mid_out\",\"a.loop_in_mid\",\"a.loop_in_out\",\"a.direct_mid_out\",\"a.direct_in_mid\",\"a.direct_in_out\")\\\n",
    "    .withColumnRenamed(\"fees\",\"fee1\")\\\n",
    "    .withColumnRenamed(\"address_recv_dist\",\"address_recv_to_mid\")\\\n",
    "    .withColumnRenamed(\"amount_recv\",\"amount_recv_to_mid\")\\\n",
    "    .withColumnRenamed(\"tx_sent\",\"tx_sent_from_in\")\\\n",
    "    .withColumnRenamed(\"address_sent_dist\",\"address_sent_from_in\")\\\n",
    "    .withColumnRenamed(\"amount_sent\",\"amount_sent_from_in\")\n",
    "\n",
    "    motifs_2_cloned.write.parquet(pathDirfinal+\"/motifs2_\"+str(i)+\".parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = load_and_get_table_df(\"kryptosare\", \"cluster\")\n",
    "\n",
    "for hh in range(0,1):\n",
    "    block1(hh)\n",
    "    block2(hh)\n",
    "    block3(hh)\n",
    "    block4(hh)\n",
    "    block5(hh)\n",
    "    block6(hh)\n",
    "    block7(hh)\n",
    "    block8(hh)\n",
    "    block9(hh)\n",
    "    block10(hh)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
