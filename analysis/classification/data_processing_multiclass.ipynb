{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.datastax.spark:spark-cassandra-connector_2.11:2.3.0  pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark')\n",
    "from pyspark import SparkContext,SparkConf\n",
    "sc = SparkContext()\n",
    "#With Cluster\n",
    "#conf = (SparkConf()\n",
    "#         .setMaster(\"spark://10.200.5.39:7077\")\n",
    "#         .set(\"spark.driver.host\",\"10.200.5.39\") \n",
    "#         .set(\"spark.executor.memory\",\"58g\")\n",
    "#         .set('spark.driver.memory', '60G')\n",
    "#         .setAppName(\"Classification\"))\n",
    "#sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import binascii\n",
    "from pyspark.sql import SQLContext\n",
    "from functools import reduce\n",
    "from sklearn import tree\n",
    "from pyspark.sql.types import *\n",
    "#import pygraphviz\n",
    "import pyspark.sql.functions as f\n",
    "#import networkx as nx\n",
    "#from networkx.readwrite import json_graph\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn import datasets,metrics\n",
    "from sklearn.model_selection import StratifiedKFold,KFold,train_test_split\n",
    "import numpy as np\n",
    "import json\n",
    "#import tensorflow as tf\n",
    "from IPython.display import Image\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "#from networkx.drawing.nx_pydot import write_dot\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score  \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report,f1_score,accuracy_score,precision_score,recall_score,matthews_corrcoef\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import pickle\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Kriptosare.class\"\n",
    "path_directory=\"/analysis/analysis_rest/final\"\n",
    "modelpath='/Kriptosare.class/train_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cSchema  = StructType([\n",
    "    StructField(\"user\", StringType(), True),\n",
    "    StructField(\"label\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "cSchema2  = StructType([\n",
    "    StructField(\"user\", StringType(), True),\n",
    "    StructField(\"predict_class1\", FloatType(), True),\n",
    "    StructField(\"predict_class2\", FloatType(), True),\n",
    "    StructField(\"predict_class3\", FloatType(), True),\n",
    "    StructField(\"predict_class4\", FloatType(), True),\n",
    "    StructField(\"predict_class5\", FloatType(), True),\n",
    "    StructField(\"predict_class6\", FloatType(), True)\n",
    "])\n",
    "\n",
    "def join_dataframe_intag(DF,pref):\n",
    "    DF=DF.groupBy(\"user\").agg(f.collect_list(\"label\").alias(\"list\"))\n",
    "    DF = DF.select(\"user\",f.explode(\"list\").alias(\"value\")).groupBy(\"user\",\"value\").agg(f.count(\"value\").alias(\"cnt\"))\n",
    "    DF2 = DF.groupBy(\"user\").agg(f.count(\"user\").alias(\"cnt\")).drop(\"cnt\")\n",
    "    DF2 = DF2.alias(\"a\").join(DF.alias('b'),(f.col(\"a.user\")==f.col(\"b.user\")) & (f.col(\"b.value\")==1),\"leftouter\")\\\n",
    "    .select(\"a.user\",\"b.cnt\").withColumnRenamed(\"cnt\",pref+\"_cnt1\")\n",
    "    DF2 = DF2.alias(\"a\").join(DF.alias('b'),(f.col(\"a.user\")==f.col(\"b.user\")) & (f.col(\"b.value\")==2),\"leftouter\")\\\n",
    "    .select(\"a.user\",pref+\"_cnt1\",\"b.cnt\").withColumnRenamed(\"cnt\",pref+\"_cnt2\")\n",
    "    DF2 = DF2.alias(\"a\").join(DF.alias('b'),(f.col(\"a.user\")==f.col(\"b.user\")) & (f.col(\"b.value\")==3),\"leftouter\")\\\n",
    "    .select(\"a.user\",pref+\"_cnt1\",pref+\"_cnt2\",\"b.cnt\").withColumnRenamed(\"cnt\",pref+\"_cnt3\")\n",
    "    DF2 = DF2.alias(\"a\").join(DF.alias('b'),(f.col(\"a.user\")==f.col(\"b.user\")) & (f.col(\"b.value\")==4),\"leftouter\")\\\n",
    "    .select(\"a.user\",pref+\"_cnt1\",pref+\"_cnt2\",pref+\"_cnt3\",\"b.cnt\").withColumnRenamed(\"cnt\",pref+\"_cnt4\")\n",
    "    DF2 = DF2.alias(\"a\").join(DF.alias('b'),(f.col(\"a.user\")==f.col(\"b.user\")) & (f.col(\"b.value\")==5),\"leftouter\")\\\n",
    "    .select(\"a.user\",pref+\"_cnt1\",pref+\"_cnt2\",pref+\"_cnt3\",pref+\"_cnt4\",\"b.cnt\").withColumnRenamed(\"cnt\",pref+\"_cnt5\")\n",
    "    DF2 = DF2.alias(\"a\").join(DF.alias('b'),(f.col(\"a.user\")==f.col(\"b.user\")) & (f.col(\"b.value\")==6),\"leftouter\")\\\n",
    "    .select(\"a.user\",pref+\"_cnt1\",pref+\"_cnt2\",pref+\"_cnt3\",pref+\"_cnt4\",pref+\"_cnt5\",\"b.cnt\").withColumnRenamed(\"cnt\",pref+\"_cnt6\")\n",
    "    DF2=DF2.fillna(0)\n",
    "    return DF2\n",
    "\n",
    "def link_outclass_intag(user,b):\n",
    "    dd = [(str(user[i][0]), int(b[i])) for i in range(len(user))]\n",
    "    df= sqlContext.createDataFrame(sc.parallelize(dd),schema=cSchema)\n",
    "    return df\n",
    "\n",
    "def link_outclass_intag_probs(user,b):\n",
    "    dd = [(str(user[i][0]), float(b[i][0]),float(b[i][1]),float(b[i][2]),float(b[i][3]),float(b[i][4]),float(b[i][5])) for i in range(len(user))]\n",
    "    df= sqlContext.createDataFrame(sc.parallelize(dd),schema=cSchema2)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_and_get_table_df(keys_space_name, table_name):\n",
    "    table_df = sqlContext.read\\\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "        .options(table=table_name, keyspace=keys_space_name)\\\n",
    "        .load()\n",
    "    return table_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Definition\n",
    "# 1: exchange\n",
    "# 2: gambling\n",
    "# 3: market\n",
    "# 4: pool\n",
    "# 5: mixer\n",
    "# 6: service\n",
    "# 7: wallet\n",
    "# 8: lending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################\n",
    "#               IMPORT DATAFRAMES\n",
    "#########################################################################\n",
    "\n",
    "entity= sqlContext.read.parquet(path+path_directory+\"/entity_feature_1.parquet\")\n",
    "address= sqlContext.read.parquet(path+path_directory+\"/address_feature_1.parquet\")\n",
    "motif1 = sqlContext.read.parquet(path+path_directory+\"/motifs1_1.parquet\")\n",
    "motif2 = sqlContext.read.parquet(path+path_directory+\"/motifs2_1.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.20.3 when using version 0.20.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.20.3 when using version 0.20.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "filename_rfc = modelpath+'/rfc_add.pkl'\n",
    "loaded_model_rfc_add = pickle.load(open(filename_rfc, 'rb'))\n",
    "\n",
    "filename_rfc = modelpath+'/rfc_mot1.pkl'\n",
    "loaded_model_rfc_mot1 = pickle.load(open(filename_rfc, 'rb'))\n",
    "\n",
    "filename_rfc = modelpath+'/rfc_mot2.pkl'\n",
    "loaded_model_rfc_mot2 = pickle.load(open(filename_rfc, 'rb'))\n",
    "\n",
    "filename_rfc = modelpath+'/final_gbc.pkl'\n",
    "loaded_model_rfc_final = pickle.load(open(filename_rfc, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store DF into Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity=entity.withColumnRenamed(\"count_recv\",\"tx_recv\")\n",
    "entity=entity.withColumnRenamed(\"count_sent\",\"tx_sent\")\n",
    "entity=entity.withColumnRenamed(\"balancerecv\",\"amount_recv\")\n",
    "entity=entity.withColumnRenamed(\"balancesend\",\"amount_sent\")\n",
    "entity=entity.drop(\"height\")\n",
    "entity.write\\\n",
    ".format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".mode('append')\\\n",
    ".options(table=\"entity\", keyspace=\"kryptosare_test\")\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "#               ADDRESS ML\n",
    "####################################################################\n",
    "\n",
    "address_feature = address.fillna(0)\n",
    "add_X=address.select(\"count_rec\",\"totamount_rec\",\"count_sent\",\"totamount_sent\",\"balance\",\"unique\",\"sibling\")\n",
    "#Round amount field\n",
    "add_X = add_X.withColumn(\"totamount_rec\", f.round(add_X[\"totamount_rec\"], 6))\n",
    "add_X = add_X.withColumn(\"totamount_sent\", f.round(add_X[\"totamount_sent\"], 6))\n",
    "add_X = add_X.withColumn(\"balance\", f.round(add_X[\"balance\"], 6))\n",
    "\n",
    "#Transform input/output dataframe in vector\n",
    "X_add = add_X.collect()\n",
    "#Reshape the output vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs=loaded_model_rfc_add.predict(X_add)\n",
    "address_name=address.select(\"address\").collect()\n",
    "df_sp=link_outclass_intag(address_name,probs)\n",
    "df_sp=df_sp.withColumnRenamed(\"user\",\"address\")\n",
    "df_sp=df_sp.withColumnRenamed(\"label\",\"class\")\n",
    "address_to_store=address.join(df_sp,\"address\")\n",
    "address_to_store=address_to_store.drop(\"height\")\n",
    "address_to_store=address_to_store.withColumnRenamed(\"count_rec\",\"tx_recv\")\n",
    "address_to_store=address_to_store.withColumnRenamed(\"count_sent\",\"tx_sent\")\n",
    "address_to_store=address_to_store.withColumnRenamed(\"totamount_rec\",\"amount_recv\")\n",
    "address_to_store=address_to_store.withColumnRenamed(\"totamount_sent\",\"amount_sent\")\n",
    "address_to_store.write\\\n",
    ".format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".mode('append')\\\n",
    ".options(table=\"address\", keyspace=\"kryptosare_test\")\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format motifs amount data (round and cluster to reduce the amount of data)\n",
    "motif11=motif1.withColumn(\"amount_recv\",(f.round(f.col(\"amount_recv\")/100000)*100000))\n",
    "motif11=motif11.withColumn(\"amount_sent\",(f.round(f.col(\"amount_sent\")/100000)*100000))\n",
    "motif11=motif11.withColumn(\"fees\",(f.round(f.col(\"fees\")/100)*100))\n",
    "motif11=motif11.groupby(\"outuser\",\"address_recv_dist\",\"address_sent_dist\",\"amount_recv\",\"tx_sent\",\"amount_sent\",\"tx_recv_tot\",\"fees\",\"loop_in_out\",\"direct_in_out\")\\\n",
    ".agg(f.count(\"outuser\").alias(\"repetition\"))\n",
    "df_motifs1=motif11.randomSplit([0.5,0.5])\n",
    "df_motifs1[0].write.parquet(path+path_directory+\"/motifs1_part1.parquet\")\n",
    "df_motifs1[1].write.parquet(path+path_directory+\"/motifs1_part2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for yy in range(1,3):\n",
    "\n",
    "    df_motifs1_part=sqlContext.read.parquet(path+path_directory+\"/motifs1_part\"+str(yy)+\".parquet\")\n",
    "    df_motifs1_part=df_motifs1_part.withColumn(\"id\",f.monotonically_increasing_id())\n",
    "    df_motifs1=df_motifs1_part.randomSplit([0.2,0.2,0.2,0.2,0.2])\n",
    "\n",
    "    X_train_mot1=[]\n",
    "    y_train_mot1=[]\n",
    "    id_list=[]\n",
    "\n",
    "    for i in range(0,5):\n",
    "        #Split the input and the output from dataframe \n",
    "        mot1_X=df_motifs1[i].select(\"address_recv_dist\",\"amount_recv\",\"tx_sent\",\"address_sent_dist\",\"amount_sent\",\"tx_recv_tot\",\"fees\",\"loop_in_out\",\"direct_in_out\",\"repetition\")\n",
    "        id_listX=df_motifs1[i].select(\"id\")\n",
    "        #Round amount field\n",
    "        mot1_X = mot1_X.withColumn(\"amount_recv\", f.round(mot1_X[\"amount_recv\"], 6))\n",
    "        mot1_X = mot1_X.withColumn(\"amount_sent\", f.round(mot1_X[\"amount_sent\"], 6))\n",
    "        mot1_X = mot1_X.withColumn(\"fees\", f.round(mot1_X[\"fees\"], 6))\n",
    "\n",
    "        #Transform input/output dataframe in vector\n",
    "        X_train_mot1_ = mot1_X.collect()\n",
    "        X_train_mot1 = X_train_mot1+X_train_mot1_\n",
    "        id_list_=id_listX.collect()\n",
    "        id_list=id_list+id_list_\n",
    "        \n",
    "    probs=loaded_model_rfc_mot1.predict(X_train_mot1)\n",
    "    df_sp=link_outclass_intag(id_list,probs)\n",
    "    df_sp=df_sp.withColumnRenamed(\"user\",\"id\")\n",
    "    df_sp=df_sp.withColumnRenamed(\"label\",\"class\")\n",
    "    motif11_to_store=df_motifs1_part.join(df_sp,\"id\")\n",
    "    uuidUdf= udf(lambda : str(uuid.uuid4()),StringType())\n",
    "    motif11_to_store_noid=motif11_to_store.drop(\"id\")\n",
    "    motif11_to_store_noid=motif11_to_store_noid.withColumn(\"id\",uuidUdf())\n",
    "    motif11_to_store_noid.write\\\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .mode('append')\\\n",
    "    .options(table=\"motifs1\", keyspace=\"kryptosare_test\")\\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format motifs amount data (round and cluster to reduce the amount of data)\n",
    "\n",
    "motif2=motif2.fillna(0,subset=[\"amount_sent_from_in\",\"fee1\"])\n",
    "motif21=motif2.withColumn(\"amount_recv_to_out\",(f.round(f.col(\"amount_recv_to_out\")/1000000)*1000000))\n",
    "motif21=motif21.withColumn(\"amount_sent_from_mid\",(f.round(f.col(\"amount_sent_from_mid\")/1000000)*1000000))\n",
    "motif21=motif21.withColumn(\"amount_recv_to_mid\",(f.round(f.col(\"amount_recv_to_mid\")/1000000)*1000000))\n",
    "motif21=motif21.withColumn(\"amount_sent_from_in\",(f.round(f.col(\"amount_sent_from_in\")/1000000)*1000000))\n",
    "\n",
    "motif21=motif21.withColumn(\"fee2\",(f.round(f.col(\"fee2\")/1000)*1000))\n",
    "motif21=motif21.withColumn(\"fee1\",(f.round(f.col(\"fee1\")/1000)*1000))\n",
    "motif21=motif21.groupby(\"outuser\",\"address_recv_dist_to_out\",\"amount_recv_to_out\",\"fee2\",\\\n",
    "                             \"tx_sent_from_mid\",\"address_sent_from_mid\",\"amount_sent_from_mid\",\\\n",
    "                             \"address_recv_to_mid\",\"amount_recv_to_mid\",\"tx_sent_from_in\",\"address_sent_from_in\",\\\n",
    "                             \"amount_sent_from_in\",\"fee1\",\"loop_mid_out\",\"loop_in_mid\",\"loop_in_out\",\\\n",
    "                             \"direct_mid_out\",\"direct_in_mid\",\"direct_in_out\")\\\n",
    ".agg(f.count(\"outuser\").alias(\"repetition\"))\n",
    "\n",
    "motif21=motif21.withColumn(\"id\",f.monotonically_increasing_id())\n",
    "df_motifs2=motif21.randomSplit([0.2,0.2,0.2,0.2,0.2])\n",
    "\n",
    "df_motifs2[0].write.parquet(path+path_directory+\"/motifs2_part1.parquet\")\n",
    "df_motifs2[1].write.parquet(path+path_directory+\"/motifs2_part2.parquet\")\n",
    "df_motifs2[2].write.parquet(path+path_directory+\"/motifs2_part3.parquet\")\n",
    "df_motifs2[3].write.parquet(path+path_directory+\"/motifs2_part4.parquet\")\n",
    "df_motifs2[4].write.parquet(path+path_directory+\"/motifs2_part5.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for yy in range(1,6):\n",
    "    df_motifs2_part=sqlContext.read.parquet(path+path_directory+\"/motifs2_part\"+str(yy)+\".parquet\")\n",
    "    df_motifs2=df_motifs2_part.randomSplit([0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1])\n",
    "\n",
    "    X_train_mot2=[]\n",
    "    y_train_mot2=[]\n",
    "    id_list=[]\n",
    "\n",
    "    for i in range(0,10):\n",
    "        #Split the input and the output from dataframe \n",
    "        mot2_X=df_motifs2[i].select(\"address_recv_dist_to_out\",\"amount_recv_to_out\",\"fee2\",\\\n",
    "                                 \"tx_sent_from_mid\",\"address_sent_from_mid\",\"amount_sent_from_mid\",\\\n",
    "                                 \"address_recv_to_mid\",\"amount_recv_to_mid\",\"tx_sent_from_in\",\"address_sent_from_in\",\\\n",
    "                                 \"amount_sent_from_in\",\"fee1\",\"loop_mid_out\",\"loop_in_mid\",\"loop_in_out\",\\\n",
    "                                 \"direct_mid_out\",\"direct_in_mid\",\"direct_in_out\",\"repetition\")\n",
    "\n",
    "        mot2_id=df_motifs2[i].select(\"id\")\n",
    "\n",
    "        #Round amount field\n",
    "        mot2_X = mot2_X.withColumn(\"amount_recv_to_out\", f.round(mot2_X[\"amount_recv_to_out\"], 6))\n",
    "        mot2_X = mot2_X.withColumn(\"amount_sent_from_mid\", f.round(mot2_X[\"amount_sent_from_mid\"], 6))\n",
    "        mot2_X = mot2_X.withColumn(\"amount_recv_to_mid\", f.round(mot2_X[\"amount_recv_to_mid\"], 6))\n",
    "        mot2_X = mot2_X.withColumn(\"amount_sent_from_in\", f.round(mot2_X[\"amount_sent_from_in\"], 6))\n",
    "        mot2_X = mot2_X.withColumn(\"fee2\", f.round(mot2_X[\"fee2\"], 6))\n",
    "        mot2_X = mot2_X.withColumn(\"fee1\", f.round(mot2_X[\"fee1\"], 6))\n",
    "\n",
    "        #Transform input/output dataframe in vector\n",
    "        X_train_mot2_ = mot2_X.collect()\n",
    "        id_list_ = mot2_id.collect()\n",
    "\n",
    "        X_train_mot2 = X_train_mot2+X_train_mot2_\n",
    "        id_list = id_list+id_list_\n",
    "\n",
    "    probs=loaded_model_rfc_mot2.predict(X_train_mot2)\n",
    "    df_sp=link_outclass_intag(id_list,probs)\n",
    "    df_sp=df_sp.withColumnRenamed(\"user\",\"id\")\n",
    "    df_sp=df_sp.withColumnRenamed(\"label\",\"class\")\n",
    "    motif21_to_store=motif21.join(df_sp,\"id\")\n",
    "\n",
    "    uuidUdf= udf(lambda : str(uuid.uuid4()),StringType())\n",
    "    motif21_to_store_noid=motif21_to_store.drop(\"id\")\n",
    "    motif21_to_store_noid=motif21_to_store_noid.withColumn(\"id\",uuidUdf())\n",
    "\n",
    "    motif21_to_store_noid.write\\\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .mode('append')\\\n",
    "    .options(table=\"motifs2\", keyspace=\"kryptosare_test\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cascading final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rfc_add= load_and_get_table_df(\"kryptosare_test\", \"address\")\n",
    "y_pred_rfc_mot1= load_and_get_table_df(\"kryptosare_test\", \"motifs1\")\n",
    "y_pred_rfc_mot2= load_and_get_table_df(\"kryptosare_test\", \"motifs2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rfc_add_array=y_pred_rfc_add.groupBy(\"user\",\"class\").agg(f.countDistinct(\"address\").alias(\"rep\"))\n",
    "y_pred_rfc_add_array=y_pred_rfc_add_array.withColumnRenamed(\"user\",\"user2\")\n",
    "\n",
    "f1=y_pred_rfc_add_array.where(f.col(\"class\")==1)\n",
    "f2=y_pred_rfc_add_array.where(f.col(\"class\")==2)\n",
    "f3=y_pred_rfc_add_array.where(f.col(\"class\")==3)\n",
    "f4=y_pred_rfc_add_array.where(f.col(\"class\")==4)\n",
    "f5=y_pred_rfc_add_array.where(f.col(\"class\")==5)\n",
    "f6=y_pred_rfc_add_array.where(f.col(\"class\")==6)\n",
    "\n",
    "y_pred_rfc_add_ent=y_pred_rfc_add.groupBy(\"user\").agg(f.first(\"user\").alias(\"user2\")).drop(\"user2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "first=y_pred_rfc_add_ent.alias(\"a\").join(f1.alias(\"b\"),f.col(\"a.user\")==f.col(\"b.user2\"),\"leftouter\").drop(\"user2\",\"class\")\\\n",
    ".withColumnRenamed(\"rep\",\"add_cnt1\")\n",
    "\n",
    "first=first.alias(\"a\").join(f2.alias(\"b\"),f.col(\"a.user\")==f.col(\"b.user2\"),\"leftouter\").drop(\"user2\",\"class\")\\\n",
    ".withColumnRenamed(\"rep\",\"add_cnt2\")\n",
    "\n",
    "first=first.alias(\"a\").join(f3.alias(\"b\"),f.col(\"a.user\")==f.col(\"b.user2\"),\"leftouter\").drop(\"user2\",\"class\")\\\n",
    ".withColumnRenamed(\"rep\",\"add_cnt3\")\n",
    "\n",
    "first=first.alias(\"a\").join(f4.alias(\"b\"),f.col(\"a.user\")==f.col(\"b.user2\"),\"leftouter\").drop(\"user2\",\"class\")\\\n",
    ".withColumnRenamed(\"rep\",\"add_cnt4\")\n",
    "\n",
    "first=first.alias(\"a\").join(f5.alias(\"b\"),f.col(\"a.user\")==f.col(\"b.user2\"),\"leftouter\").drop(\"user2\",\"class\")\\\n",
    ".withColumnRenamed(\"rep\",\"add_cnt5\")\n",
    "\n",
    "y_end_add=first.alias(\"a\").join(f6.alias(\"b\"),f.col(\"a.user\")==f.col(\"b.user2\"),\"leftouter\").drop(\"user2\",\"class\")\\\n",
    ".withColumnRenamed(\"rep\",\"add_cnt6\")\n",
    "\n",
    "y_end_add=y_end_add.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rfc_mot1_array=y_pred_rfc_mot1.groupBy(\"outuser\",\"class\").agg(f.count(\"id\").alias(\"rep\"))\n",
    "y_pred_rfc_mot1_array=y_pred_rfc_mot1_array.withColumnRenamed(\"outuser\",\"user2\")\n",
    "\n",
    "f1=y_pred_rfc_mot1_array.where(f.col(\"class\")==1)\n",
    "f2=y_pred_rfc_mot1_array.where(f.col(\"class\")==2)\n",
    "f3=y_pred_rfc_mot1_array.where(f.col(\"class\")==3)\n",
    "f4=y_pred_rfc_mot1_array.where(f.col(\"class\")==4)\n",
    "f5=y_pred_rfc_mot1_array.where(f.col(\"class\")==5)\n",
    "f6=y_pred_rfc_mot1_array.where(f.col(\"class\")==6)\n",
    "\n",
    "y_pred_rfc_mot1_array_ent=y_pred_rfc_mot1.groupBy(\"outuser\").agg(f.first(\"outuser\").alias(\"user\")).drop(\"outuser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "first=y_pred_rfc_mot1_array_ent.alias(\"a\").join(f1.alias(\"b\"),f.col(\"a.user\")==f.col(\"b.user2\"),\"leftouter\").drop(\"user2\",\"class\")\\\n",
    ".withColumnRenamed(\"rep\",\"mot1_cnt1\")\n",
    "\n",
    "first=first.alias(\"a\").join(f2.alias(\"b\"),f.col(\"a.user\")==f.col(\"b.user2\"),\"leftouter\").drop(\"user2\",\"class\")\\\n",
    ".withColumnRenamed(\"rep\",\"mot1_cnt2\")\n",
    "\n",
    "first=first.alias(\"a\").join(f3.alias(\"b\"),f.col(\"a.user\")==f.col(\"b.user2\"),\"leftouter\").drop(\"user2\",\"class\")\\\n",
    ".withColumnRenamed(\"rep\",\"mot1_cnt3\")\n",
    "\n",
    "first=first.alias(\"a\").join(f4.alias(\"b\"),f.col(\"a.user\")==f.col(\"b.user2\"),\"leftouter\").drop(\"user2\",\"class\")\\\n",
    ".withColumnRenamed(\"rep\",\"mot1_cnt4\")\n",
    "\n",
    "first=first.alias(\"a\").join(f5.alias(\"b\"),f.col(\"a.user\")==f.col(\"b.user2\"),\"leftouter\").drop(\"user2\",\"class\")\\\n",
    ".withColumnRenamed(\"rep\",\"mot1_cnt5\")\n",
    "\n",
    "y_end_mot1=first.alias(\"a\").join(f6.alias(\"b\"),f.col(\"a.user\")==f.col(\"b.user2\"),\"leftouter\").drop(\"user2\",\"class\")\\\n",
    ".withColumnRenamed(\"rep\",\"mot1_cnt6\")\n",
    "\n",
    "y_end_mot1=y_end_mot1.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rfc_mot2_array=y_pred_rfc_mot2.groupBy(\"outuser\",\"class\").agg(f.count(\"id\").alias(\"rep\"))\n",
    "y_pred_rfc_mot2_array=y_pred_rfc_mot2_array.withColumnRenamed(\"outuser\",\"user2\")\n",
    "\n",
    "f1=y_pred_rfc_mot2_array.where(f.col(\"class\")==1)\n",
    "f2=y_pred_rfc_mot2_array.where(f.col(\"class\")==2)\n",
    "f3=y_pred_rfc_mot2_array.where(f.col(\"class\")==3)\n",
    "f4=y_pred_rfc_mot2_array.where(f.col(\"class\")==4)\n",
    "f5=y_pred_rfc_mot2_array.where(f.col(\"class\")==5)\n",
    "f6=y_pred_rfc_mot2_array.where(f.col(\"class\")==6)\n",
    "\n",
    "y_pred_rfc_mot2_array_ent=y_pred_rfc_mot2.groupBy(\"outuser\").agg(f.first(\"outuser\").alias(\"user\")).drop(\"outuser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "first=y_pred_rfc_mot2_array_ent.alias(\"a\").join(f1.alias(\"b\"),f.col(\"a.user\")==f.col(\"b.user2\"),\"leftouter\").drop(\"user2\",\"class\")\\\n",
    ".withColumnRenamed(\"rep\",\"mot2_cnt1\")\n",
    "\n",
    "first=first.alias(\"a\").join(f2.alias(\"b\"),f.col(\"a.user\")==f.col(\"b.user2\"),\"leftouter\").drop(\"user2\",\"class\")\\\n",
    ".withColumnRenamed(\"rep\",\"mot2_cnt2\")\n",
    "\n",
    "first=first.alias(\"a\").join(f3.alias(\"b\"),f.col(\"a.user\")==f.col(\"b.user2\"),\"leftouter\").drop(\"user2\",\"class\")\\\n",
    ".withColumnRenamed(\"rep\",\"mot2_cnt3\")\n",
    "\n",
    "first=first.alias(\"a\").join(f4.alias(\"b\"),f.col(\"a.user\")==f.col(\"b.user2\"),\"leftouter\").drop(\"user2\",\"class\")\\\n",
    ".withColumnRenamed(\"rep\",\"mot2_cnt4\")\n",
    "\n",
    "first=first.alias(\"a\").join(f5.alias(\"b\"),f.col(\"a.user\")==f.col(\"b.user2\"),\"leftouter\").drop(\"user2\",\"class\")\\\n",
    ".withColumnRenamed(\"rep\",\"mot2_cnt5\")\n",
    "\n",
    "y_end_mot2=first.alias(\"a\").join(f6.alias(\"b\"),f.col(\"a.user\")==f.col(\"b.user2\"),\"leftouter\").drop(\"user2\",\"class\")\\\n",
    ".withColumnRenamed(\"rep\",\"mot2_cnt6\")\n",
    "\n",
    "y_end_mot2=y_end_mot2.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the information of the prediction in each dataframe\n",
    "col=[\"add_cnt1\",\"add_cnt2\",\"add_cnt3\",\"add_cnt4\",\"add_cnt5\",\"add_cnt6\"]\n",
    "   \n",
    "y_end_add = y_end_add.withColumn(\"sum\",sum([y_end_add[c] for c in col]))\n",
    "for c in col:\n",
    "    y_end_add = y_end_add.withColumn(c,f.round(f.col(c)/f.col(\"sum\")*100,2))\n",
    "       \n",
    "########################################### \n",
    "col=[\"mot1_cnt1\",\"mot1_cnt2\",\"mot1_cnt3\",\"mot1_cnt4\",\"mot1_cnt5\",\"mot1_cnt6\"]\n",
    "    \n",
    "y_end_mot1= y_end_mot1.withColumn(\"sum\",sum([y_end_mot1[c] for c in col]))\n",
    "for c in col:\n",
    "    y_end_mot1 = y_end_mot1.withColumn(c,f.round(f.col(c)/f.col(\"sum\")*100,2))\n",
    "    \n",
    "###########################################   \n",
    "col=[\"mot2_cnt1\",\"mot2_cnt2\",\"mot2_cnt3\",\"mot2_cnt4\",\"mot2_cnt5\",\"mot2_cnt6\"]\n",
    "y_end_mot2 = y_end_mot2.withColumn(\"sum\",sum([y_end_mot2[c] for c in col]))\n",
    "for c in col:\n",
    "    y_end_mot2 = y_end_mot2.withColumn(c,f.round(f.col(c)/f.col(\"sum\")*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rfc_add=y_end_add.drop(\"sum\")\n",
    "y_pred_rfc_mot1=y_end_mot1.drop(\"sum\")\n",
    "y_pred_rfc_mot2=y_end_mot2.drop(\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join all the dataframe information into one\n",
    "df_final_rf = entity.join(y_pred_rfc_add,['user'],\"outer\")\n",
    "df_final_rf = df_final_rf.join(y_pred_rfc_mot1,['user'],\"outer\")\n",
    "df_final_rf = df_final_rf.join(y_pred_rfc_mot2,['user'],\"outer\")\n",
    "df_final_rf = df_final_rf.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create final dataframe, split into X and y and convert them into list\n",
    "rf_final_X=df_final_rf.select(\"amount_recv\",\"amount_sent\",\"balance\",\"tx_recv\",\"tx_sent\",\"add_recv\",\"add_sent\",\\\n",
    "                        \"add_cnt1\",\"add_cnt2\",\"add_cnt3\",\"add_cnt4\",\"add_cnt5\",\"add_cnt6\",\\\n",
    "                        \"mot1_cnt1\",\"mot1_cnt2\",\"mot1_cnt3\",\"mot1_cnt4\",\"mot1_cnt5\",\"mot1_cnt6\",\\\n",
    "                        \"mot2_cnt1\",\"mot2_cnt2\",\"mot2_cnt3\",\"mot2_cnt4\",\"mot2_cnt5\",\"mot2_cnt6\")\n",
    "\n",
    "#Round amount field\n",
    "rf_final_X = rf_final_X.withColumn(\"amount_recv\", f.round(rf_final_X[\"amount_recv\"], 6))\n",
    "rf_final_X = rf_final_X.withColumn(\"amount_sent\", f.round(rf_final_X[\"amount_sent\"], 6))\n",
    "rf_final_X = rf_final_X.withColumn(\"balance\", f.round(rf_final_X[\"balance\"], 6))\n",
    "\n",
    "#Transform input/output dataframe in vector\n",
    "rf_final_X_full= rf_final_X.fillna(0).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs=loaded_model_rfc_final.predict_proba(rf_final_X_full)\n",
    "entityname=df_final_rf.select(\"user\").collect()\n",
    "df_sp=link_outclass_intag_probs(entityname,probs)\n",
    "entity_to_store=df_final_rf.join(df_sp,\"user\")\n",
    "entity_to_store=entity_to_store.drop(\"height\")\n",
    "\n",
    "entity_to_store.write\\\n",
    ".format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".mode('append')\\\n",
    ".options(table=\"enriched_entity\", keyspace=\"kryptosare_test\")\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
