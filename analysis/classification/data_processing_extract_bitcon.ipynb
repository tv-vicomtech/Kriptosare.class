{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.datastax.spark:spark-cassandra-connector_2.11:2.3.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark')\n",
    "from pyspark import SparkContext,SparkConf\n",
    "sc = SparkContext()\n",
    "#with Cluster\n",
    "#conf = (SparkConf()\n",
    "#         .setMaster(\"spark://10.200.5.39:7077\")\n",
    "#         .set(\"spark.driver.host\",\"10.200.5.39\") \n",
    "#         .set(\"spark.executor.memory\",\"58g\")\n",
    "#         .set('spark.driver.memory', '60G')\n",
    "#         .setAppName(\"readApp\"))\n",
    "#sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import binascii\n",
    "from pyspark.sql import SQLContext\n",
    "from functools import reduce\n",
    "import pygraphviz\n",
    "import pyspark.sql.functions as f\n",
    "from IPython.display import Image\n",
    "from networkx.drawing.nx_pydot import write_dot\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/Kriptosare.class/analysis/bitcoin/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from Cassandra\n",
    "def load_and_get_table_df(keys_space_name, table_name):\n",
    "    table_df = sqlContext.read\\\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "        .options(table=table_name, keyspace=keys_space_name)\\\n",
    "        .load()\n",
    "    return table_df\n",
    "\n",
    "# Convert bytearray to str\n",
    "def using_str_format(test_obj) -> str:\n",
    "    return \"\".join(\"{:02x}\".format(x) for x in test_obj)\n",
    "\n",
    "# Retrive a user in the output address dataframe\n",
    "def user_find(x):\n",
    "    return df_output_addresses_tag_grpby_addr.where(f.col(\"address\")==str(x)).select(\"user\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data, read from cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "df_blocks_original = load_and_get_table_df(\"blockchain_data\", \"block\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions_original = load_and_get_table_df(\"blockchain_data\", \"transaction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split_col = f.split(df_blocks.select('txs'), ',')\n",
    "df_blocks = df_blocks_original.withColumn('txs',f.explode(f.col('txs')))\n",
    "df_blocks = df_blocks.withColumn('txs',f.hex(f.col('txs')))\n",
    "df_blocks = df_blocks.withColumn('block_hash',f.hex(f.col('block_hash')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trasform transaction dataframe in order to separate different value in VOUT field\n",
    "df_transactions = df_transactions_original.withColumn('tx_hash',f.hex(f.col('tx_hash')))\n",
    "df_transactions = df_transactions.withColumn('tx_id',f.hex(f.col('tx_id')))\n",
    "df_transactions = df_transactions.withColumn(\"vout\", f.explode(f.col('vout')))\n",
    "\n",
    "df_transactions = df_transactions.withColumn(\"amount\",df_transactions['vout']['value']) \n",
    "df_transactions = df_transactions.withColumn(\"vout_idx\",df_transactions['vout']['n']) \n",
    "df_transactions = df_transactions.withColumn(\"address\",f.explode(df_transactions['vout']['addresses'])) \n",
    "df_transactions = df_transactions.drop(\"vout\")\n",
    "\n",
    "df_transactions = df_transactions.withColumn(\"vin\", f.explode(f.col('vin')))\n",
    "df_transactions = df_transactions.withColumn(\"vin_txid\",f.hex(df_transactions['vin']['txid'])) \n",
    "df_transactions = df_transactions.withColumn(\"vin_vout\",df_transactions['vin']['vout']) \n",
    "df_transactions = df_transactions.drop(\"vin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    df_transactions_original_part2 = df_transactions.filter((f.col(\"height\")>=(i*10000)&(f.col(\"height\")<(i+1)*10000)))\n",
    "    df_transactions_original_part2.write\\\n",
    "    .format(\"com.databricks.spark.csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"codec\", \"org.apache.hadoop.io.compress.GzipCodec\")\\\n",
    "    .save(path+\"transaction_part\"+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
