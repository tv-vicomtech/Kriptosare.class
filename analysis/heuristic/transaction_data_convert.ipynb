{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = 'pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part2 la parte que vamos a convertir (la que guardamos). part1 la parte con la que vamos a hacer el join (el 'apoyo')\n",
    "with open('/Kriptosare.class/tmp/part1.txt','r') as file:\n",
    "    part1=int(file.read())        \n",
    "with open('/Kriptosare.class/tmp/part2.txt','r') as file:\n",
    "    part2=int(file.read())  \n",
    "with open('/Kriptosare.class/tmp/turno_bloque.txt','r') as file:\n",
    "    algo=file.read()\n",
    "if part1>15:\n",
    "    appname=\"part\"+list(str(part1))[0]+'_'+list(str(part1))[1]+\"+\"+\"part\"+str(part2)+\"+bloque_codigo\"+algo\n",
    "else:\n",
    "    appname=\"part\"+str(part1)+\"+\"+\"part\"+str(part2)+\"+bloque_codigo\"+algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark')\n",
    "from pyspark import SparkContext,SparkConf\n",
    "\n",
    "#with cluster\n",
    "#conf = (SparkConf()\n",
    "#         .setMaster(\"spark://10.200.5.39:7077\")\n",
    "#         .set(\"spark.driver.host\",\"10.200.5.39\") \n",
    "#         .set(\"spark.executor.memory\",\"58g\")\n",
    "#         .set('spark.driver.memory', '58G')\n",
    "#         .setAppName(appname))\n",
    "#sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Kriptosare.class/analysis\"\n",
    "pathDir =\"transaction_data_convert_complete/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import binascii\n",
    "from pyspark.sql import SQLContext\n",
    "from functools import reduce\n",
    "#import pygraphviz\n",
    "import pyspark.sql.functions as f\n",
    "from IPython.display import Image\n",
    "#from networkx.drawing.nx_pydot import write_dot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from pyspark.sql import Row\n",
    "import math\n",
    "import subprocess\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from IPython.display import HTML, Javascript, display\n",
    "\n",
    "def initialize():\n",
    "    display(HTML(\n",
    "        '''\n",
    "            <script>\n",
    "                code_show = false;\n",
    "                function restart_run_all(){\n",
    "                    IPython.notebook.kernel.restart();\n",
    "                    setTimeout(function(){\n",
    "                        IPython.notebook.execute_all_cells();\n",
    "                    }, 15000)\n",
    "                }\n",
    "            document.getElementById(\"demo\").innerHTML = restart_run_all();\n",
    "            </script>\n",
    "            <p id=\"demo\"></p>  \n",
    "\n",
    "        '''\n",
    "    ))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "def initialize():\n",
    "    display_html(\"<script>IPython.notebook.kernel.restart()</script>\",raw=True)\n",
    "    sc.stop()\n",
    "    !sleep 15\n",
    "    !jupyter nbconvert --execute transaction_data_convert_auto.ipynb --ExecutePreprocessor.timeout=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class load_parts():   \n",
    "    def __init__(self,part1,part2):\n",
    "        if part1<15:\n",
    "            name1=str(part1)\n",
    "        else:\n",
    "            name1=list(str(part1))[0]+'_'+list(str(part1))[1]\n",
    "\n",
    "            \n",
    "        self.dfs_list=[]\n",
    "        self.dfs_list.append(sqlContext.read\\\n",
    "                     .format(\"csv\")\\\n",
    "                     .option(\"header\", \"true\")\\\n",
    "                     .option(\"inferSchema\", \"true\")\\\n",
    "                     .load(path+\"bitcoin/transaction_part\"+name1))\n",
    "                             \n",
    "        self.dfs_list.append(sqlContext.read\\\n",
    "                     .format(\"csv\")\\\n",
    "                     .option(\"header\", \"true\")\\\n",
    "                     .option(\"inferSchema\", \"true\")\\\n",
    "                     .load(path+\"bitcoin/transaction_part\"+str(part2)))\n",
    "    \n",
    "    def load(self):        \n",
    "        return self.dfs_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs_list=load_parts(part1,part2).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Por bloques. \n",
    "\n",
    "def generic_initial_df_convert(dfs_list):\n",
    "    #hasta aquÃ­\n",
    "    with open('/Kriptosare.class/tmp/turno_bloque.txt','r') as file:\n",
    "        turno_bloque=file.read()\n",
    "    if turno_bloque=='1':\n",
    "        bloque1(dfs_list)\n",
    "        initialize()\n",
    "        return\n",
    "    elif turno_bloque=='2':\n",
    "        bloque2(dfs_list)                \n",
    "        initialize()\n",
    "        return\n",
    "    else:\n",
    "        bloque3()\n",
    "        if part1==11 and part2==12:\n",
    "            with open('/Kriptosare.class/tmp/checkpoint2.txt','w') as file:\n",
    "                file.write('finished')\n",
    "            os._exit(00)\n",
    "            return \n",
    "        initialize()\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def initial_df_convert(df_transactions_part):\n",
    "    df_out=df_transactions_part.select('vin_txid','vin_vout','tx_hash','address','timestamp','amount')\n",
    "    df_out=df_out.withColumnRenamed('address','address_llegada')\n",
    "    df_qin=df_transactions_part.select('address','vout_idx','tx_hash').withColumnRenamed('tx_hash','tx_hash1')\n",
    "    df_qin=df_qin.withColumnRenamed('address','address_salida').dropDuplicates() \n",
    "    return df_qin\\\n",
    "        .join(df_out, (df_out.vin_txid == df_qin.tx_hash1)&(df_out.vin_vout==df_qin.vout_idx),how='right')\\\n",
    "        .drop('vin_txid','vin_vout','vout_idx','tx_hash1').sort('timestamp').dropDuplicates() \n",
    "\n",
    "\n",
    "def initial_df_convert_no_null(df_transactions_part):\n",
    "    df_out=df_transactions_part.select('vin_txid','vin_vout','tx_hash','address','timestamp','amount')\n",
    "    df_out=df_out.withColumnRenamed('address','address_llegada')\n",
    "    df_qin=df_transactions_part.select('address','vout_idx','tx_hash').withColumnRenamed('tx_hash','tx_hash1')\n",
    "    df_qin=df_qin.withColumnRenamed('address','address_salida').dropDuplicates()     \n",
    "    return df_qin\\\n",
    "        .join(df_out, (df_out.vin_txid == df_qin.tx_hash1)&(df_out.vin_vout==df_qin.vout_idx))\\\n",
    "        .drop('tx_hash1','vout_idx').sort('timestamp').dropDuplicates()  \n",
    "\n",
    "\n",
    "def bloque1(dfs_list):\n",
    "    with open('/Kriptosare.class/tmp/bloque_actual.txt','w') as file:\n",
    "                file.write('bloque1') \n",
    "    df_prueba=initial_df_convert(dfs_list[1].filter(f.col('coinbase')==True))\\\n",
    "            .union(initial_df_convert_no_null(dfs_list[1]).drop('vin_txid','vin_vout'))\n",
    "    \n",
    "    df_miner=dfs_list[1].filter(f.col('coinbase')==True).withColumnRenamed('address','address1')\\\n",
    "    .withColumnRenamed('tx_hash','tx_hash1')\n",
    "                #print(df_miner.count(),1)\n",
    "            \n",
    "        \n",
    "    df_null_as=initial_df_convert(dfs_list[1]).filter(f.col('address_salida').isNull())\n",
    "                #print(df_null_as.count())\n",
    "            \n",
    "    \n",
    "    df_null_as=df_null_as.join(df_miner,(df_miner.tx_hash1==df_null_as.tx_hash)\\\n",
    "                &(df_miner.address1==df_null_as.address_llegada),how='left')\n",
    "        \n",
    "        \n",
    "    df_null_as=df_null_as.filter(df_null_as.address1.isNull())\\\n",
    "                    .dropDuplicates().select('address_llegada','tx_hash').withColumnRenamed('tx_hash','tx_hash1')\n",
    "        \n",
    "                #df_null_aux=df_null_as\n",
    "\n",
    "\n",
    "    df_null_as=df_null_as.join(dfs_list[1],(df_null_as.address_llegada==dfs_list[1].address)\\\n",
    "                                    &(df_null_as.tx_hash1==dfs_list[1].tx_hash)).drop('tx_hash1','address_llegada')\\\n",
    "                .dropDuplicates()\n",
    "        \n",
    "                #print(df_null_as.count(),2)\n",
    "    with open('/Kriptosare.class/tmp/checkpoint1.txt','w') as file:\n",
    "                file.write('primer parquet sin hacer') \n",
    "    try:\n",
    "        df_prueba.write.parquet(path+pathDir+\"transaction_data_convert_part\"+str(part2)+\".parquet\")\n",
    "    except Exception:\n",
    "        df_save_copy=sqlContext.read.parquet(path+pathDir+\"transaction_data_convert_part\"+str(part2)+\".parquet\")\n",
    "        df_save_copy.write.parquet(path+pathDir+\"transaction_data_convert_part_backup.parquet\",mode='overwrite')\n",
    "    \n",
    "    with open('/Kriptosare.class/tmp/checkpoint1.txt','w') as file:\n",
    "                file.write('empieza el df_null_as') \n",
    "    \n",
    "    df_null_as.write.parquet(path+\"transaction_data_convert_complete_auxiliars/df_null_as.parquet\",mode='overwrite')\n",
    "    \n",
    "    with open('/Kriptosare.class/tmp/checkpoint1.txt','w') as file:\n",
    "                file.write('df_null_as escrito')\n",
    "    \n",
    "    with open('/Kriptosare.class/tmp/turno_bloque.txt','w') as file:\n",
    "        file.write('2')\n",
    "\n",
    "    \n",
    "def bloque2(dfs_list):\n",
    "    \n",
    "    df_save_copy=sqlContext.read.parquet(path+pathDir+\"transaction_data_convert_part\"+str(part2)+\".parquet\")\n",
    "    df_save_copy.write.parquet(path+pathDir+\"transaction_data_convert_part_backup.parquet\",mode='overwrite')\n",
    "    \n",
    "    df_null_as=sqlContext.read.parquet(path+\"transaction_data_convert_complete_auxiliars/df_null_as.parquet\")\n",
    "    \n",
    "    with open('/Kriptosare.class/tmp/bloque_actual.txt','w') as file:\n",
    "                file.write('bloque2') \n",
    "    \n",
    "            \n",
    "\n",
    "    df_aux=initial_df_convert_no_null(dfs_list[0].union(df_null_as))\n",
    "    df_aux.write.parquet(path+\"transaction_data_convert_complete_auxiliars/df_aux.parquet\",mode='overwrite')\n",
    "    with open('/Kriptosare.class/tmp/turno_bloque.txt','w') as file:\n",
    "            file.write('3')\n",
    "\n",
    "def bloque3():\n",
    "    with open('/Kriptosare.class/tmp/bloque_actual.txt','w') as file:\n",
    "                file.write('bloque3') \n",
    "    dfs_renamed=dfs_list[1].withColumnRenamed('address','address1').withColumnRenamed('tx_hash','tx_hash1')\\\n",
    "                .withColumnRenamed('timestamp','timestamp1').withColumnRenamed('amount','amount1')     \n",
    "    \n",
    "    df_aux=sqlContext.read.parquet(path+\"transaction_data_convert_complete_auxiliars/df_aux.parquet\")\n",
    "        \n",
    "    df_aux=df_aux.join(dfs_renamed,(dfs_renamed.tx_hash1==df_aux.tx_hash)\\\n",
    "                &(dfs_renamed.address1==df_aux.address_llegada)&(dfs_renamed.vin_vout==df_aux.vin_vout)\\\n",
    "                &(dfs_renamed.vin_txid==df_aux.vin_txid)&(dfs_renamed.amount1==df_aux.amount))\\\n",
    "                .select('address_salida','tx_hash','address_llegada','timestamp','amount')\n",
    "                    #print(df_aux.count(),3)\n",
    "            \n",
    "    with open('/Kriptosare.class/tmp/checkpoint2.txt','w') as file:\n",
    "                file.write('segundo parquet sin hacer')                    \n",
    "    df_aux.write.parquet(path+pathDir+\"transaction_data_convert_part\"+str(part2)+\".parquet\",mode='append')\n",
    "    \n",
    "    \n",
    "    \n",
    "    with open('/Kriptosare.class/tmp/checkpoint2.txt','w') as file:\n",
    "                file.write('segundo parquet hecho')\n",
    "    \n",
    "    if part1==part2-1:\n",
    "        with open('/Kriptosare.class/tmp/part1.txt','w') as file:\n",
    "            file.write('1')\n",
    "                \n",
    "        with open('/Kriptosare.class/tmp/part2.txt','w') as file:\n",
    "            file.write(str(part2+1))\n",
    "                \n",
    "        with open('/Kriptosare.class/tmp/turno_bloque.txt','w') as file:\n",
    "            file.write('1')\n",
    "    else:\n",
    "        with open('/Kriptosare.class/tmp/part1.txt','w') as file:\n",
    "            file.write(str(part1+1))\n",
    "        with open('/Kriptosare.class/tmp/turno_bloque.txt','w') as file:\n",
    "            file.write('2')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_initial_df_convert(dfs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
