{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['JAVA_HOME'] = \"/usr/lib/jvm/java-1.8.0-openjdk-amd64\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = ' --packages com.datastax.spark:spark-q-connector_2.11:2.3.0 --packages com.datastax.spark:spark-cassandra-connector_2.11:2.3.2 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Kriptosare.class/tmp/trans_part.txt','r') as file:\n",
    "    trans_part=int(file.read())\n",
    "with open(\"/Kriptosare.class/tmp/turno_bloque.txt\",'r') as file:\n",
    "    turno_bloque=int(file.read())\n",
    "if trans_part>15:\n",
    "    appname=\"part\"+str(trans_part)[0]+'_'+str(trans_part)[1:]+\"+bloque_codigo\"+str(turno_bloque)\n",
    "else:\n",
    "    appname=\"part\"+str(trans_part)+\"+bloque_codigo\"+str(turno_bloque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark')\n",
    "from pyspark import SparkContext,SparkConf\n",
    "sc = SparkContext()\n",
    "#with cluster\n",
    "#conf = (SparkConf()\n",
    "#         .setMaster(\"spark://10.200.5.39:7077\")\n",
    "#         .set(\"spark.driver.host\",\"10.200.5.39\") \n",
    "#         .set(\"spark.executor.memory\",\"58g\")\n",
    "#         .set('spark.driver.memory', '58G')\n",
    "#         .set('spark.driver.maxResultSize','4G')\n",
    "#         .set('spark.rpc.message.maxSize','512')\n",
    "#         .setAppName(appname))\n",
    "#sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import binascii\n",
    "from pyspark.sql import SQLContext\n",
    "from functools import reduce\n",
    "#import pygraphviz\n",
    "import pyspark.sql.functions as f\n",
    "from IPython.display import Image\n",
    "#from networkx.drawing.nx_pydot import write_dot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from pyspark.sql import Row\n",
    "import math\n",
    "import pickle\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, Javascript, display\n",
    "\n",
    "def initialize():\n",
    "    display(HTML(\n",
    "        '''\n",
    "            <script>\n",
    "                code_show = false;\n",
    "                function restart_run_all(){\n",
    "                    IPython.notebook.kernel.restart();\n",
    "                    setTimeout(function(){\n",
    "                        IPython.notebook.execute_all_cells();\n",
    "                    }, 15000)\n",
    "                }\n",
    "            document.getElementById(\"demo\").innerHTML = restart_run_all();\n",
    "            </script>\n",
    "            <p id=\"demo\"></p>  \n",
    "\n",
    "        '''\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from IPython.display import display_html\\ndef initialize():\\n    display_html(\"<script>IPython.notebook.kernel.restart()</script>\",raw=True)\\n    sc.stop()\\n    !sleep 15\\n    !jupyter nbconvert --execute Heuristics.ipynb --ExecutePreprocessor.timeout=-1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from IPython.display import display_html\n",
    "def initialize():\n",
    "    display_html(\"<script>IPython.notebook.kernel.restart()</script>\",raw=True)\n",
    "    sc.stop()\n",
    "    !sleep 15\n",
    "    !jupyter nbconvert --execute Heuristics.ipynb --ExecutePreprocessor.timeout=-1'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Kriptosare.class/analysis\"\n",
    "pathDir =\"transaction_data_convert_complete/\"\n",
    "pathAux=\"heuristic_auxiliars/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristico1_ad_salida(aux):\n",
    "    aux0=aux.groupby('address_salida').agg(f.min('user')).withColumnRenamed('address_salida','address_salida1').withColumnRenamed('min(user)','user1').sort('user1')\n",
    "    \n",
    "    aux0=aux.join(aux0,aux.address_salida==aux0.address_salida1)\\\n",
    "    .selectExpr('user1 as user','address_salida1 as address_salida','tx_hash as tx_hash','output_number as output_number')\\\n",
    "    \n",
    "\n",
    "    aux1=aux0.filter(f.col('output_number')==1).groupby(f.col('tx_hash')).agg(f.min('user'))\\\n",
    "    .withColumnRenamed('min(user)','user1').withColumnRenamed('tx_hash','tx_hash1')\n",
    "    \n",
    "    kk=aux0.join(aux1,f.col('tx_hash')==f.col('tx_hash1'),how='left')\n",
    "\n",
    "    user_relation1=kk.select('user1','user').filter(f.col('user1')!=f.col('user')).dropDuplicates()\n",
    "    \n",
    "    df_H1_salida_part1=kk.filter(f.col('user1').isNull()).selectExpr('address_salida','tx_hash','output_number','user')\n",
    "    df_H1_salida_part2=kk.filter(f.col('user1').isNotNull())\\\n",
    "    .selectExpr('address_salida','tx_hash','output_number','user1 as user')\n",
    "    kk=df_H1_salida_part1.union(df_H1_salida_part2).sort('user')\n",
    "\n",
    "    return kk.select('tx_hash','address_salida','user').dropDuplicates(),user_relation1\n",
    "\n",
    "'''def heuristico1_ad_salida(aux,part=1):\n",
    "    if part==1:\n",
    "        boolean=True\n",
    "    else:\n",
    "        boolean=False\n",
    "    aux0=aux.groupby('address_salida').agg(f.min('user')).withColumnRenamed('address_salida','address_salida1').withColumnRenamed('min(user)','user1').sort('user1')\n",
    "    \n",
    "    aux0=aux.join(aux0,aux.address_salida==aux0.address_salida1)\\\n",
    "    .selectExpr('user1 as user','address_salida1 as address_salida','tx_hash as tx_hash','output_number as output_number')\\\n",
    "    .sort('user',ascending=boolean)\n",
    "\n",
    "    aux1=aux0.filter(f.col('output_number')==1).dropDuplicates(['tx_hash'])\\\n",
    "    .selectExpr('user as user1', 'tx_hash as tx_hash1', 'address_salida as address_salida1','output_number as output_number1')\\\n",
    "    .sort('user1')\n",
    "    \n",
    "    kk=aux0.join(aux1,f.col('tx_hash')==f.col('tx_hash1'),how='left')\n",
    "\n",
    "    user_relation1=kk.select('user1','user').filter(f.col('user1')!=f.col('user')).dropDuplicates()\n",
    "    \n",
    "    df_H1_salida_part1=kk.filter(f.col('user1').isNull()).selectExpr('address_salida','tx_hash','output_number','user')\n",
    "    df_H1_salida_part2=kk.filter(f.col('user1').isNotNull())\\\n",
    "    .selectExpr('address_salida','tx_hash','output_number','user1 as user')\n",
    "    kk=df_H1_salida_part1.union(df_H1_salida_part2).sort('user')\n",
    "\n",
    "    return kk.select('tx_hash','address_salida','user').dropDuplicates(),user_relation1'''\n",
    "'''\n",
    "def heuristico1_ad_llegada(df_prueba,df_H1_salida):\n",
    "    df_H1_llegada=df_prueba.drop('address_salida','timestamp').sort('tx_hash')\n",
    "    return df_H1_llegada.join(df_H1_salida.drop('tx_hash'),df_H1_salida.address_salida==df_H1_llegada.address_llegada, how='left')\\\n",
    "        .drop('address_salida').dropDuplicates().sort('user')'''\n",
    "\n",
    "def heuristico1_ad_llegada(df_prueba,df_H1_salida):\n",
    "    df_H1_llegada=df_prueba.dropDuplicates(['address_llegada']).drop('timestamp').sort('tx_hash')\n",
    "    kk=df_H1_salida.dropDuplicates(['address_salida']).drop('tx_hash')\n",
    "    return df_H1_llegada.join(kk,kk.address_salida==df_H1_llegada.address_llegada, how='left')\\\n",
    "        .select('address_llegada','user').dropDuplicates(['address_llegada']).filter(f.col('user').isNotNull()).sort('user')\n",
    "\n",
    "def heuristico2_ad_llegada(df,df_H1_salida,part=1):\n",
    "    if part==1:\n",
    "        df_prueba=df\n",
    "        kk=df_prueba.filter(f.col('output_number')==2).filter(f.col('new_addresses_per_trans')==1)\\\n",
    "            .dropDuplicates(['address_salida'])\n",
    "    else:\n",
    "        df_prueba=df.drop('user')\n",
    "        kk=df_prueba.filter(f.col('user_llegada').isNull()).filter(f.col('output_number')==2).filter(f.col('new_addresses_per_trans')==1)\\\n",
    "            .dropDuplicates(['address_salida'])\n",
    "    df_prueba1=kk\\\n",
    "    .join(df_H1_salida,(df_H1_salida.address_salida==kk.address_salida)&(df_H1_salida.tx_hash==kk.tx_hash))\\\n",
    "    .selectExpr('address_llegada as address_llegada1','user').dropDuplicates()\n",
    "    \n",
    "    return df_prueba1.join(df_prueba,df_prueba.address_llegada==df_prueba1.address_llegada1,how='right')\\\n",
    "        .select('address_llegada','user').filter(f.col('user').isNotNull()).dropDuplicates(['address_llegada'])    \n",
    "    \n",
    "def union_heur_ad_llegada(df_H1_llegada,df_H2_llegada):\n",
    "    df_H1_aux=df_H1_llegada.selectExpr('address_llegada as address_llegada1', 'user as user1')\n",
    "    return df_H2_llegada\\\n",
    "        .join(df_H1_aux,df_H2_llegada.address_llegada==df_H1_aux.address_llegada1)\\\n",
    "        .selectExpr('address_llegada1 as address_llegada', 'user1 as user1', 'user as user').dropDuplicates().sort('user')\n",
    "\n",
    "        \n",
    "def user_per_transaction(df_prueba,part=1):      \n",
    "        #ponemos un usuario por cada transacción\n",
    "        if part==1:\n",
    "            aux=df_prueba.filter(df_prueba.address_salida.isNotNull()).drop('address_llegada').sort('timestamp','tx_hash')\n",
    "        else:\n",
    "            aux=df_prueba.filter(df_prueba.address_salida.isNotNull()).sort('timestamp','tx_hash')\n",
    "        aux=aux.collect()\n",
    "        with open(\"/Kriptosare.class/tmp/cont.txt\",'r') as file:\n",
    "            cont=int(file.read())\n",
    "        for i in range(len(aux)-1):\n",
    "            rdict=aux[i].asDict()\n",
    "            rdict['user']=\"user\"+'0'*(13-len(str(cont)))+str(cont)\n",
    "            aux[i]=Row(**rdict)\n",
    "            if aux[i]['tx_hash']!=aux[i+1]['tx_hash'] or (aux[i]['output_number']!=1):\n",
    "                cont=cont+1\n",
    "        rdict=aux[len(aux)-1].asDict()\n",
    "        rdict['user']=\"user\"+'0'*(13-len(str(cont)))+str(cont)\n",
    "        aux[len(aux)-1]=Row(**rdict)\n",
    "        aux=sqlContext.createDataFrame(aux)\n",
    "        \n",
    "        with open(\"/Kriptosare.class/tmp/cont.txt\",'w') as file:\n",
    "            file.write(str(cont+1))\n",
    "        if part==1:\n",
    "            return aux.drop('timestamp').sort('user')\n",
    "        else:\n",
    "            return aux.sort('user')\n",
    "        \n",
    "def cluster_add_with_user(df_H1_salida,df_H_llegada,user_representation):\n",
    "    df=df_H1_salida.selectExpr('address_salida as address','user')\\\n",
    "    .union(df_H_llegada.selectExpr('address_llegada as address','user')).dropDuplicates(['address'])\n",
    "    \n",
    "    df=df.join(user_representation,user_representation.user1==df.user,how='left')\n",
    "    df1=df.filter(f.col('representator').isNotNull()).select('address','representator')\n",
    "    df2=df.filter(f.col('representator').isNull()).selectExpr('address','user as representator')\n",
    "    return df1.union(df2).withColumnRenamed('representator','user').dropDuplicates()\n",
    "\n",
    "        \n",
    "def ad_restantes(df_prueba,df_H_cluster):\n",
    "    add_totales=df_prueba.selectExpr('address_salida as address1').union(df_prueba.select('address_llegada'))\\\n",
    "        .filter(f.col('address1').isNotNull()).dropDuplicates()\n",
    "    \n",
    "    return add_totales.join(df_H_cluster,f.col('address1')==f.col('address'),how='left')\\\n",
    "        .filter(f.col('address').isNull()).selectExpr('address1 as address').dropDuplicates()      \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\"\"\"def ad_restantes(df_prueba):\n",
    "    with open(\"/home/hadoop/Heuristics/cont.txt\",'r') as file:\n",
    "        cont=int(file.read())\n",
    "    \n",
    "    aux2=df_prueba.filter(df_prueba.address_salida.isNull()).drop('address_salida').dropDuplicates(['address_llegada'])\n",
    "    aux2=aux2.collect()\n",
    "    for i in range(len(aux2)):\n",
    "        cont=cont+1\n",
    "        rdict=aux2[i].asDict()\n",
    "        rdict['user']=\"user\"+'0'*(10-len(str(cont)))+str(cont)\n",
    "        aux2[i]=Row(**rdict)\n",
    "        \n",
    "    with open(\"/home/hadoop/Heuristics/cont.txt\",'w') as file:\n",
    "        file.write(str(cont+1))\n",
    "    \n",
    "    aux2=sqlContext.createDataFrame(aux2)\n",
    "    return aux2.selectExpr('address_llegada as address', 'user as user')\n",
    "\"\"\"\n",
    "def ad_restantes_raw(address):\n",
    "    with open(\"/Kriptosare.class/tmp/cont.txt\",'r') as file:\n",
    "        cont=int(file.read())\n",
    "    aux2=address.collect()\n",
    "    for i in range(len(aux2)):\n",
    "        cont=cont+1\n",
    "        rdict=aux2[i].asDict()\n",
    "        rdict['user']=\"user\"+'0'*(13-len(str(cont)))+str(cont)\n",
    "        aux2[i]=Row(**rdict)\n",
    "    with open(\"/Kriptosare.class/tmp/cont.txt\",'w') as file:\n",
    "        file.write(str(cont+1))\n",
    "    \n",
    "    aux2=sqlContext.createDataFrame(aux2)\n",
    "    return aux2\n",
    "        \n",
    "def user_repr(df_H_llegada=True,add_user_rel=True):\n",
    "    #Pequeño cambio para combinar df_H_llegada y user_relation para crear otro user_relation\n",
    "    if add_user_rel==True:\n",
    "        user_relation=df_H_llegada.drop('address_llegada').dropDuplicates().sort('user')\n",
    "    else:\n",
    "        if df_H_llegada!=True:\n",
    "            user_relation=df_H_llegada.drop('address_llegada').union(add_user_rel).dropDuplicates().sort('user')\n",
    "        else:\n",
    "            user_relation=add_user_rel\n",
    "    urel_list=user_relation.filter(f.col('user').isNotNull()).filter(f.col('user1').isNotNull()).dropDuplicates().collect()\n",
    "#Cambios aquí\n",
    "    urel_list1=[]\n",
    "    h={}\n",
    "    rep_list=[]\n",
    "    for i in range(len(urel_list)):\n",
    "        rdict=urel_list[i].asDict()        \n",
    "        if rdict['user1']!=rdict['user']:             \n",
    "            if (rdict['user1'] not in h.keys()) and (rdict['user'] not in h.keys()):\n",
    "                h[rdict['user']]={rdict['user1'],rdict['user']}\n",
    "                h[rdict['user1']]=rdict['user']\n",
    "                rep_list.append(rdict['user'])\n",
    "\n",
    "            elif (rdict['user1'] in h.keys()) and (rdict['user'] not in h.keys()):\n",
    "\n",
    "                u1=rel_degree(h,rdict['user1'])\n",
    "                h[u1].add(rdict['user'])\n",
    "                h[rdict['user']]=u1  \n",
    "\n",
    "\n",
    "            elif (rdict['user'] in h.keys()) and (rdict['user1'] not in h.keys()):\n",
    "\n",
    "                u=rel_degree(h,rdict['user'])\n",
    "                h[u].add(rdict['user1'])\n",
    "                h[rdict['user1']]=u\n",
    "\n",
    "            else:\n",
    "                u1=rel_degree(h,rdict['user1'])\n",
    "                u=rel_degree(h,rdict['user'])\n",
    "                if u1==u:\n",
    "                    pass\n",
    "                else:\n",
    "                    if len(h[u1])>len(h[u]):\n",
    "                        h[u1]=h[u1].union(h[u])\n",
    "                        h[u]=u1\n",
    "                        rep_list.remove(u)\n",
    "                    else:\n",
    "                        h[u]=h[u].union(h[u1])\n",
    "                        h[u1]=u\n",
    "                        rep_list.remove(u1)\n",
    "    for i in rep_list:\n",
    "        for j in h[i]:        \n",
    "            urel_list1.append(Row(representator=i,user1=j))\n",
    "    return sqlContext.createDataFrame(urel_list1)\n",
    "\n",
    "def rel_degree(h,key):\n",
    "    a=key\n",
    "    l=[a]\n",
    "    c=0\n",
    "    if type(h[a])==type(set()):\n",
    "        return a\n",
    "    while type(h[a])!=type(set()):       \n",
    "        c+=1\n",
    "        a=h[a]\n",
    "        if (a not in l):\n",
    "            l.append(a)\n",
    "        else:\n",
    "            print('hay un bucle',a,c)\n",
    "            return\n",
    "    return a\n",
    "\n",
    "def column_add(df_prueba,part=1):\n",
    "    df1=df_prueba.groupby(f.col('tx_hash')).agg(f.countDistinct('address_llegada'))\\\n",
    "    .withColumnRenamed('tx_hash','tx_hash1').withColumnRenamed('count(DISTINCT address_llegada)','output_number')\n",
    "    df1=df_prueba.join(df1,df_prueba.tx_hash==df1.tx_hash1,how='left').drop('tx_hash1').sort('timestamp')\n",
    "    if part==1:\n",
    "        df2=df1.dropDuplicates(['address_llegada']).groupby(f.col('tx_hash')).agg(f.countDistinct('address_llegada'))\\\n",
    "        .withColumnRenamed('count(DISTINCT address_llegada)','new_addresses_per_trans').withColumnRenamed('tx_hash','tx_hash1')\n",
    "    else:\n",
    "        df2=df1.filter(f.col('user_llegada').isNull()).dropDuplicates(['address_llegada']).groupby(f.col('tx_hash')).agg(f.countDistinct('address_llegada'))\\\n",
    "        .withColumnRenamed('count(DISTINCT address_llegada)','new_addresses_per_trans').withColumnRenamed('tx_hash','tx_hash1')\\\n",
    "        \n",
    "    return df1.join(df2,df1.tx_hash==df2.tx_hash1,how='left')\\\n",
    "        .withColumn(\"new_addresses_per_trans\", f.when(f.col(\"new_addresses_per_trans\").isNull(), 0)\\\n",
    "        .otherwise(f.col(\"new_addresses_per_trans\"))).drop('tx_hash1')\n",
    "    \n",
    "def delete_file(some_path):\n",
    "    subprocess.call([\"hdfs\", \"dfs\", \"-rm\", \"-r\", some_path])\n",
    "def rename_file(some_path,name):\n",
    "    subprocess.call([\"hdfs\", \"dfs\", \"-mv\", some_path, name])\n",
    "    \n",
    "def spliting_func(df_no_user):\n",
    "    l=sqlContext.read.parquet(path+pathDir+\"transaction_data_convert_part\"+str(trans_part)+\".parquet\")\\\n",
    "        .select('timestamp').dropDuplicates().sort('timestamp').collect()\n",
    "    min_val=l[0][0]\n",
    "    max_val=l[len(l)-1][0]\n",
    "    N=df_no_user.count()\n",
    "    n=math.floor(N/21343813)\n",
    "    df_no_users=[]\n",
    "    k=min_val\n",
    "    delta_x=max_val-min_val\n",
    "    for i in range(n):\n",
    "        if i<n-1:\n",
    "            df_no_users.append(df_no_user.filter((f.col('timestamp')>=k)&(f.col('timestamp')<k+(delta_x/2))))\n",
    "            k+=(delta_x/2)\n",
    "            delta_x=delta_x/2\n",
    "            \n",
    "        else:\n",
    "            df_no_users.append(df_no_user.filter(f.col('timestamp')>=k))\n",
    "    \n",
    "    big_aux=user_per_transaction(df_no_users[0].sort('timestamp','tx_hash'),0)\n",
    "    \n",
    "    for i in range(1,len(df_no_users)):\n",
    "        big_aux=big_aux.union(user_per_transaction(df_no_users[i].sort('timestamp','tx_hash'),0))\n",
    "    return big_aux.dropDuplicates().sort('user')\n",
    "\n",
    "def spliting_add_raw(addresses):\n",
    "    n=math.floor(addresses.count()/21343813)\n",
    "    df=addresses.withColumn('counter',f.monotonically_increasing_id())\n",
    "    max_val=df.agg(f.max('counter')).collect()[0][0]\n",
    "    \n",
    "    min_val=df.agg(f.min('counter')).collect()[0][0]\n",
    "    address_split=[]\n",
    "    for i in range(n):\n",
    "        if i<n-1:\n",
    "            address_split.append(df.filter((f.col('counter')>=min_val+(max_val-min_val)*i/n)&(f.col('counter')<min_val+(max_val-min_val)*(i+1)/n)))\n",
    "        else:\n",
    "            address_split.append(df.filter((f.col('counter')>=min_val+(max_val-min_val)*i/n)&(f.col('counter')<=min_val+(max_val-min_val)*(i+1)/n)))\n",
    "    big_aux2=ad_restantes_raw(address_split[0])       \n",
    "    for i in range(1,len(address_split)):\n",
    "        big_aux2=big_aux2.union(ad_restantes_raw(address_split[i]))\n",
    "    return big_aux2.drop('counter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic_clustering_no_restrictions():\n",
    "        df=sqlContext.read.parquet(path+pathDir+\"transaction_data_convert_part1.parquet\")\n",
    "        df_prueba=column_add(df)\n",
    "            \n",
    "        #ponemos un usuario por cada transacción\n",
    "        with open(\"/Kriptosare.class/tmp/checkpoint.txt\",'w') as file:\n",
    "                file.write('user_per_transaction')\n",
    "        aux=user_per_transaction(df_prueba)\n",
    "        \n",
    "        \n",
    "        #pasamos heurístico 1 a los address_salida\n",
    "        with open(\"/Kriptosare.class/tmp/checkpoint.txt\",'w') as file:\n",
    "                file.write('heuristico1_ad_salida')\n",
    "        df_H1_salida,user_relation=heuristico1_ad_salida(aux)  \n",
    "        \n",
    "        #hacemos que los address_llegada que están en la salida de alguna transacción reciban el user(hereden):\n",
    "        with open(\"/Kriptosare.class/tmp/checkpoint.txt\",'w') as file:\n",
    "                file.write('heuristico1_ad_llegada')\n",
    "        df_H1_llegada=heuristico1_ad_llegada(df_prueba,df_H1_salida)\n",
    "        \n",
    "        #Pasamos el heurístico 2 en address_llegada:\n",
    "        with open(\"/Kriptosare.class/tmp/checkpoint.txt\",'w') as file:\n",
    "                file.write('heuristico2_ad_llegada')\n",
    "        df_H2_llegada=heuristico2_ad_llegada(df_prueba,df_H1_salida)\n",
    "        \n",
    "        \n",
    "        #Unimos los heuristicos en address_llegada para sacar relaciones:\n",
    "        with open(\"/Kriptosare.class/tmp/checkpoint.txt\",'w') as file:\n",
    "                file.write('union_heur_ad_llegada')\n",
    "        df_H_llegada=union_heur_ad_llegada(df_H1_llegada,df_H2_llegada)\n",
    "        \n",
    "        \n",
    "        #Sacamos la relacion de users para así obtener un único representante para cada user:\n",
    "        with open(\"/Kriptosare.class/tmp/checkpoint.txt\",'w') as file:\n",
    "                file.write('user_repr')\n",
    "        user_representation=user_repr(df_H_llegada,user_relation)    \n",
    "        \n",
    "        \n",
    "        #Con las relaciones obtenidas hacemos un cluster para las address_salida\n",
    "        with open(\"/Kriptosare.class/tmp/checkpoint.txt\",'w') as file:\n",
    "                file.write('cluster_add_with_user')\n",
    "                \n",
    "        df_H_cluster=cluster_add_with_user(df_H1_salida,df_H_llegada,user_representation)\n",
    "        \n",
    "        #Unimos ambos clusters y lo guardamos     \n",
    "        with open(\"/Kriptosare.class/tmp/checkpoint.txt\",'w') as file:\n",
    "                file.write('ad_restantes')\n",
    "                \n",
    "        addresses=ad_restantes(df_prueba,df_H_cluster) \n",
    "        \n",
    "        with open(path+\"txt_files\\\\checkpoint.txt\",'w') as file:\n",
    "                file.write('ad_restantes_raw')\n",
    "                \n",
    "        aux2=ad_restantes_raw(addresses)\n",
    "        \n",
    "        with open(\"/Kriptosare.class/tmp/checkpoint.txt\",'w') as file:\n",
    "                file.write('save')\n",
    "                \n",
    "        df_H_cluster=df_H_cluster.union(aux2).sort('user').dropDuplicates(['address'])\n",
    "        \n",
    "        df_H_cluster.write\\\n",
    "                    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "                    .mode('append')\\\n",
    "                    .options(table=\"cluster\", keyspace=\"kryptosare\")\\\n",
    "                    .save()\n",
    "\n",
    "        \n",
    "#def heuristic_clustering_no_restrictions_partial_process(df_prueba,df_H1_salida):\n",
    "        \n",
    "        \n",
    "        #hacemos que los address_llegada que están en la salida de alguna transacción reciban el user(hereden):\n",
    "#        df_H1_llegada=heuristico1_ad_llegada(df_prueba,df_H1_salida)\n",
    "\n",
    "        #Pasamos el heurístico 2 en address_llegada:\n",
    "#        df_H2_llegada=heuristico2_ad_llegada(df_prueba,df_H1_salida)\n",
    "\n",
    "        \n",
    "        #Unimos los heuristicos en address_llegada para sacar relaciones:\n",
    "#        df_H_llegada=union_heur_ad_llegada(df_H1_llegada,df_H2_llegada)\n",
    "\n",
    "        #Si la parte no es la primera sólo necesitamos el user_relation y address_salida con sus user\n",
    "        #user_relation=df_H_llegada.drop('address_llegada').dropDuplicates().sort('user')\n",
    "#        return df_H_llegada.drop('address_llegada').dropDuplicates().sort('user'),df_H2_llegada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering():\n",
    "    \n",
    "    if trans_part==1:\n",
    "        with open(\"/Kriptosare.class/tmp/app_name.txt\",'w') as file:\n",
    "                file.write('part'+str(trans_part)+'+'+'bloque_código'+str(turno_bloque))\n",
    "        heuristic_clustering_no_restrictions()\n",
    "        with open(\"/Kriptosare.class/tmp/trans_part.txt\",'w') as file:\n",
    "            file.write(str(trans_part+1))\n",
    "    else:\n",
    "        with open(\"/Kriptosare.class/tmp/app_name.txt\",'w') as file:\n",
    "                file.write('part'+str(trans_part)+'+'+'bloque_código'+str(turno_bloque))\n",
    "        eval('bloque'+str(turno_bloque)+'()') \n",
    "        if trans_part==55 and turno_bloque==9:\n",
    "            with open(\"/Kriptosare.class/tmp/checkpoint.txt\",'w') as file:\n",
    "                file.write('finished')\n",
    "            os.__exit(00)\n",
    "            return\n",
    "        \n",
    "        if turno_bloque<9:\n",
    "            with open(\"/Kriptosare.class/tmp/turno_bloque.txt\",'w') as file:\n",
    "                file.write(str(turno_bloque+1))\n",
    "        else:\n",
    "            with open(\"/Kriptosare.class/tmp/turno_bloque.txt\",'w') as file:\n",
    "                file.write('1')\n",
    "                \n",
    "    initialize()   \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "def bloque1():        \n",
    "        #joins:2\n",
    "        with open(\"/Kriptosare.class/tmp/checkpoint.txt\",'w') as file:\n",
    "                file.write('running')\n",
    "        df_prueba=sqlContext.read.parquet(path+pathDir+\"transaction_data_convert_part\"+str(trans_part)+\".parquet\").dropDuplicates()\n",
    "        df_cluster_part=sqlContext.read\\\n",
    "                .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "                .options(table='cluster', keyspace='kryptosare')\\\n",
    "                .load()\\\n",
    "                .withColumnRenamed('address','address1')\n",
    "        #df_cluster_part.show()\n",
    "            \n",
    "        df_prueba_step=df_prueba.join(df_cluster_part,df_cluster_part.address1==df_prueba.address_salida,how='left')\\\n",
    "            .drop('address1').withColumnRenamed('user','user1')\n",
    "    \n",
    "            \n",
    "        df_prueba_step=df_prueba_step\\\n",
    "        .join(df_cluster_part,df_cluster_part.address1==df_prueba_step.address_llegada,how='left')\\\n",
    "        .drop('address1').withColumnRenamed('user','user_llegada').withColumnRenamed('user1','user')\n",
    "        \n",
    "        column_add(df_prueba_step,0).write.parquet(path+pathAux+'df_prueba_step.parquet',mode='overwrite')\n",
    "        \n",
    "def bloque2():\n",
    "        df_prueba_step=sqlContext.read.parquet(path+pathAux+'df_prueba_step.parquet').dropDuplicates()\n",
    "    \n",
    "        df_no_miner=df_prueba_step.filter(f.col('address_salida').isNotNull()).sort('timestamp','user',ascending=[True,False])\n",
    "\n",
    "        #pasamos el H1 a la tabla con los user actuales para obtener posibles relaciones y añadir users en una misma transacción\n",
    "        \n",
    "        df_no_user=df_no_miner.filter(f.col('user').isNull()).drop('user')\n",
    "        \n",
    "        aux=user_per_transaction(df_no_user,0)\\\n",
    "        .select('address_salida','tx_hash','address_llegada','timestamp','amount','output_number','new_addresses_per_trans','user','user_llegada')\n",
    "        \n",
    "        aux=aux.union(df_no_miner.filter(f.col('user').isNotNull()))\n",
    "        \n",
    "        aux.write.parquet(path+pathAux+'aux.parquet',mode='overwrite')\n",
    "        df_no_miner.write.parquet(path+pathAux+'df_no_miner.parquet',mode='overwrite')\n",
    "\n",
    "        \n",
    "def bloque3():\n",
    "        aux=sqlContext.read.parquet(path+pathAux+'aux.parquet')\n",
    "        df_no_miner=sqlContext.read.parquet(path+pathAux+'df_no_miner.parquet')\n",
    "\n",
    "        \n",
    "        df_no_miner=df_no_miner.filter(f.col('user').isNotNull()).union(aux)\n",
    "        \n",
    "        df_H1_salida,user_relation=heuristico1_ad_salida(df_no_miner)\n",
    "        \n",
    "        \n",
    "        \n",
    "        df_no_miner.write.parquet(path+pathAux+'apoyo1.parquet',mode='overwrite')\n",
    "        user_relation.write.parquet(path+pathAux+'user_relation.parquet',mode='overwrite')\n",
    "        df_H1_salida.write.parquet(path+pathAux+'df_H1_salida.parquet',mode='overwrite')       \n",
    "        \n",
    "def bloque4():      \n",
    "    df_no_miner=sqlContext.read.parquet(path+pathAux+'apoyo1.parquet') \n",
    "    df_H1_salida=sqlContext.read.parquet(path+pathAux+'df_H1_salida.parquet')\n",
    "    \n",
    "    df_H1_llegada=heuristico1_ad_llegada(df_no_miner.drop('user'),df_H1_salida.drop('output_number'))\n",
    "    \n",
    "    df_H1_llegada.write.parquet(path+pathAux+'df_H1_llegada.parquet',mode='overwrite')\n",
    "    \n",
    "def bloque5():\n",
    "    df_no_miner=sqlContext.read.parquet(path+pathAux+'apoyo1.parquet')\n",
    "    df_H1_salida=sqlContext.read.parquet(path+pathAux+'df_H1_salida.parquet')\n",
    "    \n",
    "    df_H2_llegada=heuristico2_ad_llegada(df_no_miner,df_H1_salida,part=0)\n",
    "        \n",
    "    df_H2_llegada.write.parquet(path+pathAux+'df_H2_llegada.parquet',mode='overwrite')   \n",
    "\n",
    "    \n",
    "def bloque6():\n",
    "    df_H1_llegada=sqlContext.read.parquet(path+pathAux+'df_H1_llegada.parquet')\n",
    "    df_H2_llegada=sqlContext.read.parquet(path+pathAux+'df_H2_llegada.parquet')\n",
    "    \n",
    "    df_H_llegada=union_heur_ad_llegada(df_H1_llegada,df_H2_llegada)\n",
    "    \n",
    "    df_H_llegada.write.parquet(path+pathAux+'df_H_llegada.parquet',mode='overwrite')\n",
    "    \n",
    "def bloque7():\n",
    "    df_H1_salida=sqlContext.read.parquet(path+pathAux+'df_H1_salida.parquet')\n",
    "    df_H_llegada=sqlContext.read.parquet(path+pathAux+'df_H_llegada.parquet')\n",
    "    user_relation=sqlContext.read.parquet(path+pathAux+'user_relation.parquet')\n",
    "\n",
    "    user_representation=user_repr(df_H_llegada,user_relation)  \n",
    "        \n",
    "                \n",
    "    df_H_cluster=cluster_add_with_user(df_H1_salida,df_H_llegada,user_representation)\n",
    "    \n",
    "    user_representation.write.parquet(path+pathAux+'user_representation.parquet',mode='overwrite')\n",
    "    df_H_cluster.write.parquet(path+pathAux+'df_H_cluster.parquet',mode='overwrite')\n",
    "        \n",
    "def bloque8():                \n",
    "    df_prueba=sqlContext.read.parquet(path+pathDir+\"transaction_data_convert_part\"+str(trans_part)+\".parquet\").dropDuplicates()\n",
    "    df_H_cluster=sqlContext.read.parquet(path+pathAux+'df_H_cluster.parquet')\n",
    "        \n",
    "    addresses=ad_restantes(df_prueba,df_H_cluster)\n",
    "    aux2=ad_restantes_raw(addresses)\n",
    "\n",
    "    \n",
    "    aux2.write.parquet(path+pathAux+'aux2.parquet',mode='overwrite')\n",
    "    \n",
    "def bloque9():\n",
    "    user_representation=sqlContext.read.parquet(path+pathAux+'user_representation.parquet').withColumnRenamed('user','user1')\n",
    "    aux2=sqlContext.read.parquet(path+pathAux+'aux2.parquet').withColumnRenamed('user','user1')\n",
    "    df_H_cluster=sqlContext.read.parquet(path+pathAux+'df_H_cluster.parquet')\n",
    "    old_cluster=sqlContext.read\\\n",
    "                .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "                .options(table='cluster', keyspace='kryptosare')\\\n",
    "                .load()\n",
    "    \n",
    "    new_cluster=df_H_cluster.union(aux2).sort('user').dropDuplicates(['address'])\n",
    "    \n",
    "    \n",
    "    #df_cluster_part=old_cluster.join(user_representation,user_representation.user1==old_cluster.user,how='left')\n",
    "    #old_cluster_no_update=df_cluster_part.filter(f.col('representator').isNull()).select('address','user')\n",
    "    old_cluster_update=old_cluster.join(user_representation,user_representation.user1==old_cluster.user).selectExpr('address','representator as user')\n",
    "    \n",
    "    \n",
    "    old_cluster_update.union(new_cluster).dropDuplicates(['address'])\\\n",
    "    .write.parquet(path+pathAux+'cluster_apoyo.parquet',mode='overwrite')\n",
    "    cluster=sqlContext.read.parquet(path+pathAux+'cluster_apoyo.parquet')\n",
    "    \n",
    "    cluster\\\n",
    "                    .write\\\n",
    "                    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "                    .mode('append')\\\n",
    "                    .options(table=\"cluster\", keyspace=\"kryptosare\")\\\n",
    "                    .save()\n",
    "    \n",
    "    with open('/Kriptosare.class/tmp/trans_part.txt','w') as file:\n",
    "                file.write(str(trans_part+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
